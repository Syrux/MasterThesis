% EPL master thesis cover template
\documentclass{eplmastersthesis}

% Imports
\usepackage{multicol}
\usepackage{multirow}
\usepackage{rotating}
\usepackage[hidelinks]{hyperref}
\usepackage{float}
\usepackage{forest}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{setspace}
\usepackage{titlesec}
\setcounter{secnumdepth}{4}
\usepackage{todonotes}
\usepackage[]{algorithm2e}
\usepackage{subcaption}
\usepackage[acronym, nonumberlist,style=list]{glossaries}
\usepackage[nottoc,numbib]{tocbibind}

\graphicspath{{F:/Memoire/CodeSavedOnline/New/MasterThesis/paper/SecondVersion/Graphs/}}

% Fill in here the information: title, student name, speciality, jury members
\title{CP based Sequence Mining on the cloud using spark}	% Master thesis title
%\subtitle{Subtitle (optional)}			% Optional subtitle
\author{Cyril \textsc{de Vogelaere}}	% Student name
%\secondauthor{Firstname \textsc{Lastname}}	% Second student name if applicable
\speciality{Computer Science}		
% Speciality (use one of the following options):
										% Biomedical Engineering
										% Chemical and Materials Engineering
										% Civil Engineering
										% Computer Science
										% Computer Science and Engineering
										% Electrical Engineering
										% Electro-mechanical Engineering
										% Mathematical Engineering
										% Mechanical Engineering
										% Physical Engineering
%\options{Option(s)}		% If required by program commission mention options
\supervisor{Pierre \textsc{Schaus}}	% 1st supervisor name
%\cosupervisor{Firstname \textsc{Lastname}}	% 2nd supervisor name if applicable
\readerone{John \textsc{Aoga}}		% 1st reader name
\readertwo{Guillaume \textsc{Derval}}		% 2nd reader name
\readerthree{Nijssen \textsc{Siegfried}}	% 3rd reader name
\readerfour{Roberto \textsc{D'Ambrosio}}	% 4rd reader name

\years{2016-2017}	% Academic year
\pagenumbering{arabic}

\newlength\myindent % define a new length \myindent
\setlength\myindent{6em} % assign the length 2em to \myindet
\newcommand\bindent{%
  \begingroup % starts a group (to keep changes local)
  \setlength{\itemindent}{\myindent} % set itemindent (algorithmic internally uses a list) to the value of \mylength
  \addtolength{\algorithmicindent}{\myindent} % adds \mylength to the default indentation used by algorithmic
}
\newcommand\eindent{\endgroup} % closes a group

\newcommand\danger{%
 \makebox[1.4em][c]{%
 \makebox[0pt][c]{\raisebox{.1em}{\small!}}%
 \makebox[0pt][c]{\color{red}\Large$\bigtriangleup$}}}%
 

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

%\renewcommand{\baselinestretch}{1.15}

\makeglossaries
\newglossaryentry{latex}
{
    name=latex,
    description={Is a mark up language specially suited 
    for scientific documents}
}
\newacronym{ppic}{PPIC}{Prefix Projection Incremental Counting propagator}
\newacronym{SoS}{SoS}{sequence of symbols}
\newacronym{SoSS}{SoSS}{sequence of sets of symbols}
\newacronym{RDD}{RDD}{resilient distributed dataset}
\newacronym{SPM}{SPM}{sequential pattern mining}
\newacronym{CP}{CP}{constraint programming}

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}

\begin{document}

\begin{singlespacing}
\maketitle					% To create front cover page
% To fix footnotes problem
\newgeometry{top=2.5cm,bottom=2.5cm,left=2.5cm,right=2.5cm}
\end{singlespacing}

\tableofcontents
\newpage
\listoffigures
\listoftables
\newpage
\listofalgorithms
\newpage

\begin{abstract}
In this paper we propose a novel constraint programming based sequential pattern mining algorithm designed to perform large-scale data mining through parallel computations in a scalable environment. In this endeavour, we will adapt the recently designed algorithm PPIC, which managed to outperform other state-of-the-art implementations by using ideas from both data mining and CP on a generic constraint solver, to support parallel computation using Spark. We will then show through detailed experiment that, by using a generic map-reduce based scalable PrefixSpan implementation to divide our original sequential pattern mining problem in sufficiently small sub-problems, and by then solving those sub-problems locally using an underlying CP framework, great performance can be obtained in a scalable architecture without losing much flexibility on the supported constraints. 
\end{abstract}

\section{Introduction}

Sequential pattern mining is a widely studied problem concerned with discovering frequent sub-sequences in a sequence database. This data mining problem has broad applications, including but not limited to: Analysis of customer purchase patterns, web-log mining, medical research, DNA sequencing, text-mining, human mobility mining, and so on\cite{mabroukeh2010taxonomy}. \newline

First introduced by Agrawal and Srikant \cite{agrawal1995mining} in 1995, sequential pattern mining problems have since been widely studied for more efficient way to find their combinatorially explosive number of possible subsequence patterns. Through the years, many techniques have thus been proposed to efficiently solve this problem, among which we must mention apriori, GSP, SPADE, and PrefixSpan \cite{agrawal1995mining, srikant1996mining, han2001prefixspan, zaki2001spade} for being millstones in the advancement of this research. \newline

Additionally, constraint programming (CP) has often been proposed as a framework for sequential pattern mining problems in recent years, producing various efficient algorithm such as CPSM, PP, or Gap-Seq. The main benefit of those algorithm lying in their modularity, since their implementation in CP framework allows the addition of multiple constraints to restrict the search space of our problem, and more efficiently work toward specific solutions. Those constraints encompassing a wide range of possibilities such as imposing restrictions on symbol occurrences and positions, imposing restrictions on the pattern length, ensuring the respect of regular expressions, and so on. \newline

However, it was feared that this increased flexibility came with a performance cost. Many of the developed algorithm staying uncompetitive in comparison to state-of-the-art specialised methods.
However, many recently made improvements, notably by Kemmar et al \cite{Kemmar_global, Kemmar_gap}, which had further extended this work by introducing one constraint (module) for both the pseudo-projection and the frequency pruning, and allowed CP based sequence mining algorithms to become competitive with state-of-the-art techniques such as Zaki's cSPADE \cite{zaki2001spade}. \newline

A recent paper \cite{aoga2016efficient} went even further by designing a CP based implementation that greatly out-performs state-of-the-art implementations. This PrefixSpan based algorithm called PPIC, designed to find patterns on sequences of individual symbols and implemented on the open source CP-solver OscaR \cite{oscar}, achieved those improved performances by combining ideas from pattern
mining and constraint programming. \newline

More specifically, this new algorithm improves the efficiency of computing prefix-projected databases (the usual bottle-neck of PrefixSpan based techniques) by using last-position lists such as those used in the LAPIN algorithm \cite{yang2007lapin}. This algorithm also improves the time needed to restore projected database when using a depth-first search approach through the use of trailing which avoid unnecessary copies of the data. \newline

However, the previously mentioned implementations were not ready for the recent advent of Big Data, as they were not adapted for large-scale data processing. Those implementation thus needed to evolve and support parallel computations, so that they could be executed in scalable 'big-data' environments and display further improved performances. \newline

While non-CP algorithms were quickly adapted to those new environments with the introduction of a scalable implementation based on PrefixSpan \cite{deng2014towards} and SPADE \cite{ho2000large}, CP-based approaches have yet to be adapted to support parallel computations. \newline

Our objective in this paper is thus to adapt the recently designed PPIC algorithm to efficiently support parallel computations in a scalable architecture. Additionally, we will take extra to try keeping the inherent flexibility of CP-based implementation without sacrificing performances.

\newpage
\section{Sequential Pattern Mining}

Sequence pattern mining (SPM) is a widely studied problem focusing on discovering sub-sequences in a dataset of given sequences. Each (sub) sequence being an ordered list of symbols, or sets of symbols. SPM has applications ranging from web log mining and text mining, to biological sequence analysis.

\subsection{Sequential Pattern Mining Background}

\subsubsection{Definitions and Concepts}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[]

\theoremstyle{example}
\newtheorem{example}{Example}[]

\begin{definition}{\bfseries Symbol, Item and Item-sets:}
A symbol (s) is an element of a sequence, symbols can be repeated in sequences but, in a given database, a symbol will keep the same meaning. Symbols can be grouped into sets of symbols. \\
An item (i) is the integer representation of a symbol. While symbols could technically be anything from an Object to a primitive type, items are integer representation of those symbols allowing shorter representation of the database. Similarly, items can be grouped in sets, we will refer to sets of items as Item-sets. \\
In the remainder of this paper, we will generally refer to elements of sequence as symbols, unless a representation as Item is necessary for the algorithm/concept concerned.
\end{definition}

\begin{definition}{\bfseries Sequence:}
A sequence seq = $\{s_1, s_2, ..., s_n\}$ of length N, is an ordered list of potentially repeating symbols. Those symbols can either be grouped into sets (in which case each set will be clearly separated from others using delimiters) or each symbol could be part of its own independent set. In the remained of this paper, we will distinguish between those two types of sequences by calling them either \acrlong{SoSS} (\acrshort{SoSS}) in the first case or \acrlong{SoS} (\acrshort{SoS}) in the second.
\end{definition}

\begin{example}{\bfseries Type of Sequences}
\begin{itemize}
\item $\langle$A B C A$\rangle$ is a \acrlong{SoS} (\acrshort{SoS}) of length 4 containing three different symbols. Symbol A being repeated multiple times in the sequence.
\item $\langle$(A B)(C A)$\rangle$ is a \acrlong{SoSS} (\acrshort{SoSS}), where parenthesis act as delimiters between symbol sets.
\item $\langle$(A)(C)(A)$\rangle$ is a \acrlong{SoS} (\acrshort{SoS}), since no sets of symbols contain more than one element.
\end{itemize}
\end{example}

\begin{definition}{\bfseries Sub-Sequence / Super-Sequence:}
A sequence alpha = $\{\alpha_1, \alpha_2, ..., \alpha_m\}$ is a subsequence of seq = $\{s_1, s_2, ..., s_n\}$ and seq is a super-sequence of alpha if and only if: m $\leq$ n and $\forall i \in \{1, ..., m\} \quad \exists j_i \quad such \quad that \quad 1 \leq j_1 \leq ... \leq j_m \leq n \quad and \quad \alpha_i = seq_{j_i}$
\end{definition}

\begin{definition}{\bfseries Sequence database:}
A sequence database is a non-ordered collection of set of tuples (sid, seq). Where 'sid' is a sequence identifier and 'seq' a sequence. If a sequence database contains only sequences of symbols, we shall denote it as SDB in the remained of this paper. In any other case we shall refer to the database with using the acronym SSDB. \\
Any algorithm mining sequential pattern for an SSDB should find the same solutions for SDB datasets as an SDB only algorithm. SDB specialised algorithms may never be used on SSDB. \\
NB: SDB are SSDB but the opposite is not true ! 
\end{definition}

\begin{example}{\bfseries Type of Sequences Database}
\begin{itemize}
\item ($\langle$A B C A$\rangle$, $\langle$D E C B$\rangle$) is an SDB, since it contains only sequences of symbols.
\item ($\langle$(A)(B)$\rangle$, $\langle$(D)(E)(C)(B)$\rangle$) is also an SDB, since it similarly only contains sequences of symbols, the maximal Symbol Set size being one. 
\item ($\langle$(A)(B)(C A)$\rangle$, $\langle$(D)(E)(C)(B)$\rangle$) is an SSDB, since it contains at least one sequence of sets of symbols, the first sequence of the database containing (C A), a symbol set with multiple symbols.
\end{itemize}
\end{example}

\begin{definition}{\bfseries Cover, Support, Pattern, Frequent Pattern:}
The \textbf{cover} of a sequence seq in SSDB, denoted by cover(seq), is the subset of sequences in SSDB that are a super-sequence of seq. 
The \textbf{support} of a sequence seq in SSDB, denoted nbSupportSDB(seq), is the number of sequence in the cover. 
Any sequence seq can be a \textbf{pattern} as long as it appears in the database, but we call \textbf{frequent patterns}, patterns where nbSupportSSDB(seq) $\geq \theta$, where $\theta$ is a
given minimum support threshold.
\end{definition}

\begin{definition}{\bfseries Sequential Pattern Mining (SPM)}
Given a minimum sup-port threshold $\theta$ and a sequence database SSDB, the SPM problem is to find all frequent patterns of the SSDB.
\end{definition}

\begin{definition}{\bfseries Prefix, Suffix}
Let $\alpha$ be a pattern. If a sequence  $\beta$ is a super-sequence of $\alpha$ then the prefix of $\alpha$ in $\beta$ is the smallest sequence of symbols in $\beta$ that is still a super-sequence of $\alpha$. The remains of $\beta$ that are not part of the prefix are called suffix, and can be obtained by projecting the prefix away.
\end{definition}

\begin{definition}{\bfseries Prefix Projected database}
A prefix-projected database of a prefix $\alpha$, denoted by $SDB_\alpha$, is the set of prefix-projections of all sequences in SDB that are a super-sequence of $\alpha$.
\end{definition}

\subsubsection{Existing specialised approaches}

\paragraph{apriori}

The Apriori algorithm, created in 1995 \cite{agrawal1995mining}, was designed for frequent item-sets mining in a transactional database.\newline

This breath first search (BFS) algorithm, whose performances are now surpassed by more modern techniques, finds all frequent item-sets by iteratively growing its sequential patterns. \newline

First, at the start of each iteration, all N-length candidate will be generated (N being the number of the iteration) using the result of the previous iteration and the 1-length candidates as basis. The support of those candidates will then be checked with the databases and un-frequent pattern will be deleted. Finally, frequent N-length patterns will be sent onto the next iteration, until none can be grown. \newline

While historically relevant, this algorithm was inefficiently generating all possible N-length candidates using the length N-1 candidates as basis. Among those a large amount of candidate wouldn't be frequent, and would thus waste huge amount of memory and CPU time being stocked and uselessly verified across the database. \newline

One of its good points however, lies in its ability to be used to detect association rules (Example: When A is present, B has 75\% chance of being also present), which can give indications about the general trends in the database and allow human operators to get a general understanding of the inputted dataset.

\paragraph{GSP}

The Generalised Sequential Pattern (GSP) algorithm \cite{srikant1996mining} was based on the apriori algorithm but redesigned for sequential pattern mining instead of frequent item-set mining. One of the good points of this new algorithm lied in the possibility to add time constraints that specified a minimum and/or maximum time period between adjacent elements in a pattern. \newline

As apriori, the algorithm start by detecting frequent 1-length pattern, it then proceeds with generating all 2-length patterns from there. However, since order now matters as items that should be grouped together can come from multiple transactions, there is now a lot more candidates to create. \newline

More specifically, given two items A and B, the generated candidate would be AB, BA and (AB). (AB) being the representation of those two items occurring in the same transactional time frame. Those candidates will then be projected on the database, and their support will be counted. All candidates having a lesser amount of support than the threshold $\theta$ will then be cleaned. \newline

The algorithm should then generate further candidates, but unlike apriori, the method has been complexified to become far more efficient. Instead of growing sequential patterns by adding all 1-length patterns to each of their end and creating a tremendous amount of unsupported patterns, we create those candidates more efficiently. \newline

First, for each sequential patterns of N-length found in the previous iterations, we detect its first and last N-1 item, thus creating two sub-sequential patterns by omitting an element either at the end or start. We will then create N+1-length candidates by composing sequential patterns with similar end and start sub-sequential pattern. \newline

For example, given the sequential patterns AA, (AB), AB and BA, we would be able to generate the following candidates: A(AB), (AB)A, B(AB), (AB)B, ABA, BAB, AAB, BAA. \\
Those candidates should then be pruned to remove impossible combinations found in previous iterations. In the case of our example, since BB was not retained as a 2-length sequential pattern due to being un-frequent, we can delete all candidate sequential patterns containing BB, as they similarly cannot be present. \\
We are thus left with: A(AB), (AB)A, ABA, AAB, BAA \newline

Finally, we will count the number of support for each candidates having passed the pruning phase, and remove unsupported sequential patterns. Another iteration of generating candidates, pruning and count support will then start, until no supported sequential patterns is found at the end of an iteration, or no candidates can be generated. \newline

If after counting the support of each candidate we detect that only ABA and BAA are supported, the next iteration would only create the candidate ABAA. Since it would be our only 4-length pattern, no 5-length pattern will be generated and the execution will stop having determined that all solutions were found. \newline

As you may expect, this new method to generate candidates allowed GSP to surpass apriori's efficiency by a wide margin. Since the algorithm additionally supported a wide range of new constraints allowing reduction of the search space, and was so efficient, it became a reference algorithm in frequent pattern mining.

\paragraph{PrefixSpan}

The PrefixSpan approach \cite{pei2004mining, han2001prefixspan} relies on pattern growth. As you may see in our simple example displayed in Figure \ref{ex:PrefixSpanExample}, the idea of this algorithm is to find the complete set of patterns through iteratively growing prefixes by projecting them on the database, and finding extensions. \newline

\begin{figure}[h]
  \centering
  \begin{subfigure}{\textwidth}
  	\centering
    \includegraphics[width=0.35\textwidth]{PrefixSpanOrigDB.png}
    \caption{Original database of our SPM example}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
  	\centering
    \includegraphics[width=0.9\textwidth]{PrefixSpanProjDB.png}
    \caption[Prefix projected database \& sequential pattern]{
		\tabular[t]{@{}l@{}}
			Prefix projected databases relative to 1-length sequential patterns \& \\
			final list of sequential patterns eventually extended from those 1-length prefixes
		\endtabular
	}
  \end{subfigure}
  \caption{PrefixSpan example}
  \label{ex:PrefixSpanExample}
\end{figure}

The main selling point of the algorithm lies in its ability to quickly generate candidate extensions, projecting prefixes on the database significantly reducing the time needed to find prefixes extensions, as only the suffixes need to be searched for those extensions to be found. Another selling point lies in its scalability, as this method is easily implementable under a Map-Combine-Reduce programming scheme. \newline

However, this method also has disadvantages, mainly in the fact that projecting databases is a time expensive process. Making building those projected databases the efficiency bottle-neck of PrefixSpan. \newline

Fortunately, techniques exists to break past this bottle-neck and mitigate its effects. Techniques among which we should mention the 'Bi-level projection' technique, which consists in creating a triangular matrix (S-matrix [\ref{fig:smatrix}]) registering the number of supports for all 2-length sequences that can be assembled from 1-length prefixes. A quick scan of the S-matrix then allows the detection of all supported 2-length prefixes, prefixes which can then be extended again through applying the same technique on their projected database. Thus allowing a reduction of the number of necessary prefix projections during the execution of the algorithm, since only half of the prefixes will need to be projected. \newline

A second technique we should mention is the 'Pseudo-Projection' technique which is based on keeping and maintaining a start index for each sequence. This start-index registering the lowest position at which the last projected prefix was supported, thus allowing quicker projection of extending prefixes by avoiding to re-project previous prefix elements.

\paragraph{cSPADE}

The cSPADE algorithm \cite{zaki2001spade} uses combinatorial properties to divide the original sequence pattern mining problem in smaller sub-problems that can be solved independently using simple id-list join operations. This algorithm is thus parallelisable, furthermore, the parallelisation has a linear scalability with respect to the size of the inputted database. \newline

cSPADE's first step is to compute all 1-length sequential patterns using a simple database scan. Then, we generate all 2-length sequential patterns and count the number of supporting sequences for each pair of items in a bi-dimensional matrix. Counting the number of supporting sequence being realised through another database scan by first transforming the original vertical representation of the database into an horizontal representation (see Figure \ref{ex:cspadeDatabaseRepr}).

\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.4\textwidth}
  	\centering
  	\includegraphics[width=0.95\textwidth]{cspadeVerticalRepr.png}
    \caption{Vertical representation of the database}
  \end{subfigure}
  \begin{subfigure}[t]{0.59\textwidth}
  	\centering
    \includegraphics[width=0.95\textwidth]{cspadeHorizontalRepr.png}
    \caption{Horizontal representation of the database}
  \end{subfigure}
  \caption{cSPADE database representation}
  \label{ex:cspadeDatabaseRepr}
\end{figure}

Subsequent N-length sequence patterns can then be formed by joining (N+1)-length patterns using their id-lists (list of positions in sequences for each item, see Figure \ref{fig:cspadeIDList}). The support of each item can also be easily calculated from ID-list, as we just need to count the number of different sequences in which it appears.  \newline

It is also important to note that this method of joining ID-lists is only efficient from 3-length patterns onward, as ID-lists for length 1 and 2 patterns can be extremely large and potentially wouldn't fit in memory. \newline

Of course, at the end of each round, un-frequent sequential pattern should be cleaned as to guarantee only frequent patterns will be extended. This algorithm can be executed using either breadth first or depth first search, and ends its execution once no patterns can be further extended.

\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{0.54\textwidth}
  	\centering
  	\includegraphics[width=\textwidth]{cspadeVerticalIDList1.png}
    \caption{ID-list of 1-length sequence patterns}
  \end{subfigure}
  \begin{subfigure}[t]{0.45\textwidth}
  	\centering
    \includegraphics[width=\textwidth]{cspadeVerticalIDList2.png}
    \caption{ID-list of 2-length sequence patterns}
  \end{subfigure}
  \caption{cSPADE's ID-lists}
  \label{fig:cspadeIDList}
\end{figure}

\subsubsection{Existing CP Based approaches}
\paragraph{CPSM}

The CPSM approach \cite{negrevergne2015constraint} was implemented to solve sequential pattern mining problems involving only sequences of symbols (no symbols set) using generic Constraint Programming (CP) solvers. It creates a global constraint allowing a search for frequent patterns over the database. Thanks to its integration in CP solvers, it also easily support additional constraints such as size constraints or regular-expression constraints. \newline

The global constraint, which forms the basis of this approach, is called the global exists-embedding constraint. Based on an incremental propagator (DFS), the algorithm will incrementally extend a prefix, until no extensions can be successfully projected. \newline

At each step, the solver will take care of assigning the next extensions in our search space. The constraint will then search said extensions to verify its validity. Of course, the whole sequence won't be searched, but only positions larger than the smallest position supporting the previous valid prefix in each sequence. \newline

If the extension is valid and its support exceeds the threshold $\theta$, the prefix will be considered 'embedded' in the database. The algorithm thus works by creating all prefixes through various incremental extensions and numerous backtracking, then verifying their embedding in the database. \newline

Since trying all possible extensions at each incremental step would be inefficient, the algorithm was also designed to prune the next possible extensions that should be considered by the solver, and only keep extensions that are sufficiently supported in the database. Thus, symbols that are considered unfrequent won't be branched over during the execution. This pruning is done by counting the number of support for each extension during the embedding existence verification. \newline

Although impressive for its modularity, the efficiency of this algorithm didn't quite caught up to specialised non-CP algorithms such as PrefixSpan or cSPADE. It, however, surpassed their efficiency at searching for specific solutions thanks to its modularity.

\paragraph{PP}

This second approach based on CPSM proposed a slightly different global constraint by taking ideas from prefix-projection \cite{kemmar2015prefix}. Similarly to its predecessor, this algorithm was designed to solve sequential pattern mining problems involving sequence of symbols (no sets of symbols).\newline

The main idea behind this improved constraint being that we have no need to check sequences for extensions support when they did not support sub-sequences of the current prefix. \newline

This new algorithm thus takes notes of the ID of sequences which support the current pattern so that its extensions may be projected only on those sequences. The key element of this new feature being that we keep ID of sequence and not copy of sequences, making it extremely memory and time efficient. \newline

During the extensions pruning phase, only supportive sequences will need to be checked in a similar fashion, as they are the only relevant sequences to find extensions with. \newline

While the rest of this global constraint implementation was very similar to CPSM, the prefix projection improvement allowed this new implementation to largely overtake its predecessors in efficiency. Making this new algorithm competitive with state-of-the-art non-CP method while keeping the improved modularity inherent to CP algorithms.

\paragraph{Gap-Seq}

This new algorithm introduced a global constraint adding support for time gap constraint. This allows, for example, to analyses purchase behaviours and find products usually bought by customers at regular time intervals. \newline

Similarly to its predecessor, the algorithm was designed to solve sequential patterns mining problems involving sequences of symbols, doing so by incrementally extending and verifying prefixes over multiple pass of a prefix projected database. \newline

The new global constraint, however, allow tighter search spaces where one can specify the minimal/maximal distance allowed between two symbols for them to remain solution. \newline

For this constraint to work efficiently, additional pruning was added when pruning for extending items, so that extensions which would violate the time gap constraints couldn't be taken into account.

\paragraph{PPIC}

Largely based on its predecessor, CPSM and PP, PPIC is an algorithm designed for solving sequence pattern mining problems involving sequences of symbols. Unlike Gap-seq, constraint over time gaps are not supported. \newline

This new algorithm speciality lying in its record-breaking efficiency, surpassing even state-of-the-art non-CP solver, generally by a wide margin (see Figure \ref{fig:PPICvsOther}).

PPIC's implementation is based on the PrefixSpan approach, it's execution can be separated in two stages:

\begin{enumerate}
\item \textbf{Pre-processing:} In this first stage, we first clean the received sequences from unfrequent symbols, renaming them into unique items. Three matrices are then build from the sequence database:
	\begin{enumerate}
		\item The 'first-position' matrix: A \#SDB*N sized matrix allowing O(1) jumps to the first occurrence of a given item. 
		\item The 'last-position' matrix: A  \#SDB*N sized matrix allowing O(1) check for the presence of a given item in the remains of a sequence.
		\item The 'interesting-position' matrix: A matrix with the same size as the original sequence database, but whose content is changed from the items forming those sequences, to the positions of the next 'interesting' item. That is the next position where an item last appears in a sequence. \\ Although, at first glance, this matrix may seem redundant with the last-position matrix, its purpose appears when one realises that to achieve the same goal a whole column of the last-position matrix would have to be checked. Similarly, keeping only this matrix would also be less efficient, since there would be no way to efficiently check if an item is present in the remains of a sequence. Both matrices are thus needed to achieve the greatest efficiency gain. 
	\end{enumerate}
	
	The pre-processing stage will also take care of adding multiple constraints, depending on the wishes of the user. Thus restricting the search space to fit more tightly the desired solutions and improve the algorithm's performances at reaching those specific solutions.
	
\item \textbf{Execution:} Once the pre-processing is finished, the algorithm will truly start to run. Using the three matrices, prefixes will be extended efficiently using an incremental propagator (DFS approach). An approach possible thanks to trailing, that is more efficient than BFS approach were multiple copies of the database would need to be kept, or where the prefixes would need to be re-projected. Thanks to this incremental propagation, memory consumption will thus be minimal during the execution of the algorithm. \newline

Through each step of the DFS execution, the last item of the current prefix will be projected efficiently as, similarly to CPSM, the algorithm keeps track of the minimal index after which the previous valid pattern was considered supported. Thus, only the remains of the sequence will need to be searched for confirming the validity of the new projection. Once projected, the algorithm will then prune possible solutions for the next pattern efficiently, thanks to the lastPosition list, and continue the execution. 

Additionally, to keep the increased performances of the PP algorithm, the algorithm keep notes of which sequences supports the current prefix while projecting the current extension. For further extensions of the prefix, only those sequences will thus have to be considered during any projection or pruning. Since the algorithm also keeps track of the number of support for each extension, an improvement was also made to stop searching for projections once that number of supporting sequences have been found during the prefix's projection. Since the remaining sequences definitely won't support the item and will remain irrelevant further down this branch of the search tree. \newline

Each time a valid solution is found. That is, a solution that satisfy all constraints injected in the solver. The execution will be momentarily interrupted. The solution will then be translated back to the symbols corresponding to the recorded items and saved in a result list. \newline

Once the solver determines that no further solution can be found under the specified constraints. The execution will terminate, and all resulting pattern will be returned.
\end{enumerate}

An example of a complete execution of PPIC can be found in Figure \ref{fig:ppic_exec_example}. The pseudo-code can be found in Algorithm [\ref{alg:PPIC1}, \ref{alg:PPIC2}]. \newline

\begin{figure}[!h]
%\hspace*{-1cm}
\centering
\scalebox{0.5}{
\begin{forest}
for tree={
  draw,
  minimum height=2cm,
  anchor=north,
  align=center,
  child anchor=north
},
[{Input sequences: \\ $\langle$ADBABCA$\rangle$ \\ $\langle$ABCA$\rangle$ \\ $\langle$AD$\rangle$ \\ Number of supporting sequences necessary: 2}, align=center, name=SS
  [{Pre-processed sequences: \\ $\langle$1421231$\rangle$ \\ $\langle$1231$\rangle$ \\ $\langle$14$\rangle$ \\ indexInSequence: \\ $[0, 0, 0]$ \\ supportCounter: \\ $[3, 2, 2, 2]$ \\ firstPosList: \\ $[1, 3, 6, 2]$ \\ $[1, 2, 3, 0]$ \\ $[1, 0, 0, 2]$ \\ lastPosList: \\ $[7, 5, 6, 2]$ \\ $[4, 2, 3, 0]$ \\ $[1, 0, 0, 2]$ \\ interestingPosList: \\ $[2, 5, 5, 5, 6, 7, 0]$ \\ $[2, 3, 4, 0]$ \\ $[2, 0]$}, name=PDC
  		[{Projecting 4: \\ indexInSequence: \\ $[2, 4, 2]$ \\ sequenceSupported: \\ $[1, 3]$ \\ itemSupportCounter: \\ $[1, 1, 1, 0]$ \\ Extensions supported: \\ \textrm{[]}}
  		]
  		[{Projecting 1: \\ indexInSequence: \\ $[1, 1, 1]$ \\ sequenceSupported: \\ $[1, 2, 3]$ \\ itemSupportCounter: \\ $[2, 2, 2, 2]$ \\ Extensions supported: \\ \textrm{[1, 2, 3, 4]}}
  			[{Projecting 14: \\ indexInSequence: \\ $[2, 4, 2]$ \\ sequenceSupported: \\ $[1, 3]$ \\ itemSupportCounter: \\ $[1, 1, 1, 0]$ \\ Extensions supported: \\ \textrm{[]}}
  			]
  			[{Projecting 11: \\ indexInSequence: \\ $[7, 4, 2]$ \\ sequenceSupported: \\ $[1, 2]$ \\ itemSupportCounter: \\ $[0, 0, 0, 0]$ \\ Extensions supported: \\ \textrm{[]}}
  			]
  			[{Projecting 13: \\ indexInSequence: \\ $[6, 3, 2]$ \\ sequenceSupported: \\ $[1, 2]$ \\ itemSupportCounter: \\ $[2, 0, 0, 0]$ \\ Extensions supported: \\ \textrm{[1]}}
  				[{Projecting 131: \\ indexInSequence: \\ $[7, 4, 2]$ \\ sequenceSupported: \\ $[1, 2]$ \\ itemSupportCounter: \\ $[0, 0, 0, 0]$ \\ Extensions supported: \\ \textrm{[]}}
  				]
  			]
  			[{Projecting 12: \\ indexInSequence: \\ $[3, 2, 2]$ \\ sequenceSupported: \\ $[1, 2]$ \\ itemSupportCounter: \\ $[2, 1, 2, 0]$ \\ Extensions supported: \\ \textrm{[1, 3]}}
  				[{Projecting 121: \\ indexInSequence: \\ $[4, 4, 2]$ \\ sequenceSupported: \\ $[1, 2]$ \\ itemSupportCounter: \\ $[1, 1, 1, 0]$ \\ Extensions supported: \\ \textrm{[]}}
  				]
  				[{Projecting 123: \\ indexInSequence: \\ $[6, 3, 2]$ \\ sequenceSupported: \\ $[1, 2]$ \\ itemSupportCounter: \\ $[2, 0, 0, 0]$ \\ Extensions supported: \\ \textrm{[1]}}
  					[{Projecting 1231: \\ indexInSequence: \\ $[7, 4, 2]$ \\ sequenceSupported: \\ $[1, 2]$ \\ itemSupportCounter: \\ $[0, 0, 0, 0]$ \\ Extensions supported: \\ \textrm{[]}}
  					]
  				]
  			]
  		]
  		[{Projecting 2: \\ indexInSequence: \\ $[3, 2, 2]$ \\ sequenceSupported: \\ $[1, 2]$ \\ itemSupportCounter: \\ $[2, 1, 2, 0]$ \\ Extensions supported: \\ \textrm{[1, 3]}}
  			[{Projecting 21: \\ indexInSequence: \\ $[4, 4, 2]$ \\ sequenceSupported: \\ $[1, 2]$ \\ itemSupportCounter: \\ $[1, 1, 1, 0]$ \\ Extensions supported: \\ \textrm{[]}}
  			]
  			[{Projecting 23: \\ indexInSequence: \\ $[6, 3, 2]$ \\ sequenceSupported: \\ $[1, 2]$ \\ itemSupportCounter: \\ $[2, 0, 0, 0]$ \\ Extensions supported: \\ \textrm{[1]}}
  				[{Projecting 231: \\ indexInSequence: \\ $[7, 4, 2]$ \\ sequenceSupported: \\ $[1, 2]$ \\ itemSupportCounter: \\ $[0, 0, 0, 0]$ \\ Extensions supported: \\ \textrm{[]}}
  				]
  			]
  		]
  		[{Projecting 3: \\ indexInSequence: \\ $[6, 3, 2]$ \\ sequenceSupported: \\ $[1, 2]$ \\ itemSupportCounter: \\ $[2, 0, 0, 0]$ \\ Extensions supported: \\ \textrm{[1]}}
  			[{Projecting 31: \\ indexInSequence: \\ $[7, 4, 2]$ \\ sequenceSupported: \\ $[1, 2]$ \\ itemSupportCounter: \\ $[0, 0, 0, 0]$ \\ Extensions supported: \\ \textrm{[]}}
  			]
  		]
  	  ]
  	]
  ]
]
%\node[anchor=west,align=left]  at ([xshift=-2cm]MS.west) {Level 3\\Criteria};
\end{forest}}
\caption[An example of PPIC's execution]{
	\tabular[t]{@{}l@{}}
		A simple execution of PPIC's algorithm.\\
		The solution patterns are the projected prefixes
	\endtabular
}
\label{fig:ppic_exec_example}
\end{figure}

\subsection{Parallelisation}

As said previously in the introduction, our goal is to achieve a scalable implementation based on PPIC, a CP algorithm based on the PrefixSpan approach.

\subsubsection{The Benefits of Parallelisation}

The benefits of achieving such a scalable algorithm are many. First, local algorithms are limited to the power of the computer they run on, achieving parallelism allows to break that limit and increase the overall efficiency by running on multiple computers at once. \newline

Another advantage lies in costs as, with the arrival of cloud technologies, running algorithms on clusters of small machines is gradually becoming cheaper than buying and maintaining supercomputers. \newline

For both those reasons, and the fact that a large quantity of SPM problems cannot be run on local architectures, designing implementations that are both scalable and efficient has gradually become extremely important for the industry.

\subsubsection{Tool Selection}

Since, fortunately, SPM problems are embarrassingly parallel problem, we had the opportunity to choose from a selection of widely used open-source libraries. We, however, restricted ourselves to Scala compatible framework, as the CP library supporting PPIC was implemented in this language. Rapidly, we were left to choose from two major options:

\begin{enumerate}
\item Hadoop mapreduce
\item Spark
\end{enumerate}

\paragraph{Hadoop}

Hadoop is an open-source framework that allows large-scale data processing across clusters of machines. Based on a Map-reduce programming model, this framework allows larger scale iterative computations on humongous quantities of data. At each iteration, the data is read from the distributed file system (HDFS), modified through a MapReduce, then stored back on the file system. \newline

The advantages of Hadoop thus lies in the simplicity of its usage. Aside from implementing the Map and Reduce process, Hadoop will take care of scheduling, data repartition and failure recovery. \newline

Widely used since its initial release on December 10, 2011. Hadoop slowly climbed to become one of the big standards in terms of large scale computation. We thus selected it as our potential scalable framework, discovering shortly after that an efficient implementation of PrefixSpan on Hadoop was already available on the internet.

\paragraph{Spark}

Spark is an open-source engine for large-scale data processing. Mainly reputed for its speed, ease of use, and ability to efficiently implement sophisticated problems. \newline

Originally developed at UC Berkeley in 2009, Its entire implementation revolves around an immutable read-only data structure called the \textbf{resilient distributed dataset (RDD)}. Maintained in a fault-tolerant way, those lazily computed RDD, built through deterministic coarse-grained transformation, have been designed to be efficiently distributed over a cluster of machines, allowing resolution of complex iterative problems in scalable environments. \newline

Furthermore, Spark has been designed to make use of its clusters RAM memory efficiently, allowing the engine to distance itself from slow HDD memory access. Of course, should the RAM memory be insufficient, Spark is perfectly able to run using nothing but the hard-drive. \newline

Spark's was thus an extremely valid choice from a technical standpoint and, similarly to Hadoop, we were surprised to discover an existing implementation of PrefixSpan available in Spark's machine learning library.

\paragraph{Final Choice}

As said earlier, both of those libraries already disposed of a scalable PrefixSpan implementation, and both were efficient and widely recognised framework to achieve parallelism. It was thus a matter of determining whose performances were better, and whether those implementations could be efficiently extended through CP technologies. \newline

Fortunately for us, performance comparison had already been done in a widely recognised scientific paper on Spark's RDD \cite{zaharia2012resilient}. Those performances are presented in Figure \ref{fig:hadoopVSspark}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\textwidth]{hadoopVSspark.png}
  \caption{Performance comparison of Hadoop and Spark}
  \label{fig:hadoopVSspark}
\end{figure}

As you can see, performance-wise, Spark vastly outperform Hadoop thanks to its ability to use both memory and disk for its computations. Allowing up to 100x speed-up under the right circumstances, as you can see in the logistic regression problem. According to the official website, Spark would also boast a 10x speed-up through on disk computation only, but no performance benchmarks were provided to back that claim. \newline

In terms of extensions through CP technologies, we quickly realised that Hadoop would be far less practical. Although MapReduce can be used to execute the standard PrefixSpan algorithm, and could certainly be modified to introduce CP elements, Spark can support any coarse-grained transformation with its RDDs, allowing a more precise implementation where only required transformations would be made, instead of simple sequences of Map-Combine-Reduce. \newline

\newpage
\section{Implementation of a Scalable CP Based Algorithm}

In this section, we shall present the various implementation we created in an attempt to improve Spark's original algorithm's performances. The performance of those implementations, however, will be tested in a later section.

\subsection{Spark's original implementation}

Before introducing our various implementation, let us present Spark's original algorithm. An algorithm based on the PrefixSpan approach, and can be separated in four stages:

\begin{enumerate}
\item \textbf{Pre-processing:} The goal of this stage is to replace each symbol of the sequence database by a unique item, to separate item-sets through a zero delimiter, and to clean the database from unfrequent items. \\
For example, the sequence $\langle$(ABD)(ABC)A$\rangle$ will become 0120123010, assuming only symbols A, B and C are frequent in the sequence database.
\item \textbf{Scalable execution:} The core of the algorithm. Its execution consists in extending prefixes through a three sub-stage process, starting from the empty prefix.
	\begin{enumerate}
	\item First, a large prefix is projected on the database, meaning that only the suffix of supporting sequence remain in the resulting database. When there is no prefix to project, the scalable execution comes to an end and the algorithm goes on to the next stage.
	\item Then, from the set of supporting sequences, we discover symbols that can extend the current prefix. If no such extension exists, we try the next large prefix.
	\item Finally, for each possible symbol extension, we extend the corresponding prefix. We then determine how long further expanding each extended prefix may take by calculating the projected database size. Depending on the calculated projected size and of the value of a user defined parameter, we then either further extend this prefix using another iteration of the scalable execution, or store it for use in the local execution stage. 
	\end{enumerate}
\item \textbf{Local execution:} The local execution is completely similar in its implementation to the scalable execution. Its only use is in significantly improving the algorithm's performance by calculating all extensions from a Prefix locally, instead of doing so while shuffling information around the scalable architecture. \\
This stage is only launched once all large prefixes have been extended sufficiently, making the projected databases that need to be processed to find future extensions small enough. Depending on the parameters inputted by the user, this stage may be skipped.
\item \textbf{Post-processing:} During the post-processing step, we translate back the unique items into the corresponding symbols they each represented. Then we send back the collected results to the user.
\end{enumerate}

During the prefix projection phase of the scalable and local execution, the algorithm will also detect which item-sets in the sequence comply with what has already been projected and can still be extended in some way. Storing such items positions in a 'partial projection' list. \newline
That way, if we are projecting an item-set containing multiple symbols from a prefix, until the end of that item-set, the items projection phases will know where to search possible extensions, preventing the algorithm from having to search the whole sequence. \newline
Also, should we be computing on a database of sequence of symbols, no partial start would be created and kept in memory, since the algorithm would automatically detect that those item-sets cannot be extended. \newline

When the current item-set end, the full remains of the sequence, from the earliest partial position recorded, will have to be searched for extensions. When extending the first item of the new item-set, new potential partial projection will be recorded, and the old ones will be discarded. \newline

During the prefix extension phase, the current partial projection will also be used to find extensions of the current item-set quicker. The remainder of the database will also be searched, but only for extension that starts new item-sets. \newline

An example of a fully scalable execution from this algorithm can be found in Figure \ref{fig:spark_exec_example}. A pseudo-code can also be found in Algorithm [\ref{alg:SPARK1}, \ref{alg:SPARK2}, \ref{alg:SPARK3}, \ref{alg:SPARK4}]. \newline

\begin{figure}[h]
\centering
\scalebox{0.55}{
\begin{forest}
for tree={
  draw,
  minimum height=2cm,
  anchor=north,
  align=center,
  child anchor=north
},
[{Input sequences: \\ $\langle$(ADB)(ABC)(A)$\rangle$ \\ $\langle$(ABC)A$\rangle$ \\ Number of supporting sequences necessary: 2}, align=center, name=SS
  [{Pre-processed sequences: \\ $\langle$0120123010$\rangle$ \\ $\langle$0123010$\rangle$}, name=PDC
  	[{Empty prefix projection: \\ $\langle$|0120123010$\rangle$ \\ $\langle$|0123010$\rangle$ \\ Partial starts: \\  \textrm{[]} \\ \textrm{[]} \\ Extensions found: \\ \textrm{[01, 02, 03]}},
  		[{Projecting 01: \\ $\langle$01|20123010$\rangle$ \\ $\langle$01|23010$\rangle$ \\ Partial starts: \\  \textrm{[4]} \\ \textrm{[]} \\ Extensions supported: \\ \textrm{[2, 3, 01]}},
  			[{Projecting 0101: \\ $\langle$012012301|0$\rangle$ \\ $\langle$012301|0$\rangle$ \\ Partial starts: \\  \textrm{[]} \\ \textrm{[]} \\ Extensions supported: \\ \textrm{[]}}]
  			[{Projecting 013: \\ $\langle$0120123|010$\rangle$ \\ $\langle$0123|010$\rangle$ \\ Partial starts: \\  \textrm{[]} \\ \textrm{[]} \\ Extensions supported: \\ \textrm{[01]}},
  				[{Projecting 01301: \\ $\langle$012012301|0$\rangle$ \\ $\langle$012301|0$\rangle$ \\ Partial starts: \\  \textrm{[]} \\ \textrm{[]} \\ Extensions supported: \\ \textrm{[]}}]
  			]
  			[{Projecting 012: \\ $\langle$012|0123010$\rangle$ \\ $\langle$012|3010$\rangle$ \\ Partial starts: \\  \textrm{[5]} \\ \textrm{[]} \\ Extensions supported: \\ \textrm{[3, 01]}},
  				[{Projecting 0123: \\ $\langle$0120123|010$\rangle$ \\ $\langle$0123|010$\rangle$ \\ Partial starts: \\  \textrm{[]} \\ \textrm{[]} \\ Extensions supported: \\ \textrm{[]}},
  					[{Projecting 012301: \\ $\langle$012012301|0$\rangle$ \\ $\langle$012301|0$\rangle$ \\ Partial starts: \\  \textrm{[]} \\ \textrm{[]} \\ Extensions supported: \\ \textrm{[]}}]
  				]
  				[{Projecting 01201: \\ $\langle$012012301|0$\rangle$ \\ $\langle$012301|0$\rangle$ \\ Partial starts: \\  \textrm{[]} \\ \textrm{[]} \\ Extensions supported: \\ \textrm{[]}}]
  			]
  		]
  		[{Projecting 03: \\ $\langle$0120123|010$\rangle$ \\ $\langle$0123|010$\rangle$ \\ Partial starts: \\  \textrm{[]} \\ \textrm{[]} \\ Extensions supported: \\ \textrm{[01]}},
  			[{Projecting 0301: \\ $\langle$012012301|0$\rangle$ \\ $\langle$012301|0$\rangle$ \\ Partial starts: \\  \textrm{[]} \\ \textrm{[]} \\ Extensions supported: \\ \textrm{[]}}]
  		]
  		[{Projecting 02: \\ $\langle$012|0123010$\rangle$ \\ $\langle$012|3010$\rangle$ \\ Partial starts: \\  \textrm{[5]} \\ \textrm{[]} \\ Extensions supported: \\ \textrm{[3, 01]}},
  			[{Projecting 0201: \\ $\langle$012012301|0$\rangle$ \\ $\langle$012301|0$\rangle$ \\ Partial starts: \\  \textrm{[]} \\ \textrm{[]} \\ Extensions supported: \\ \textrm{[]}}]
  			[{Projecting 023: \\ $\langle$0120123|010$\rangle$ \\ $\langle$0123|010$\rangle$ \\ Partial starts: \\  \textrm{[]} \\ \textrm{[]} \\ Extensions supported: \\ \textrm{[01]}},
  				[{Projecting 02301: \\ $\langle$012012301|0$\rangle$ \\ $\langle$012301|0$\rangle$ \\ Partial starts: \\  \textrm{[]} \\ \textrm{[]} \\ Extensions supported: \\ \textrm{[]}}]
  			]
  		]
  	]
  ]
]
%\node[anchor=west,align=left]  at ([xshift=-2cm]MS.west) {Level 3\\Criteria};
\end{forest}}
\caption[An example of Spark's execution]{
	\tabular[t]{@{}l@{}}
		A simple execution of Spark's algorithm.\\
		The solutions are the projected prefixes (except the empty prefix)
	\endtabular
}
\label{fig:spark_exec_example}
\end{figure}

\begin{itemize}
  \item[NB:] During our first analysis of Spark's pre-processing stage, we noticed a small inefficiency in the cleaning of the database's sequences. When multiple item-sets were fully cleaned of their item, the algorithm had a tendency of creating sequences of zero delimiters, as the algorithm still delimited empty item-sets.

Although the internal representation was still correct and results weren't modified, those trailing zeroes substantially slowed the algorithm down.
The performance improvement of this small correction is reflected in the annexes, Figure \ref{fig:spark_preprocessing_fix}.

Later, this small correction was proposed to Spark's community and quickly accepted into the default implementation. In the remains of this paper, we will thus consider this corrected version which has been approved by the community as the original algorithm for Spark, and compare our performance improvement with this corrected version as the basis.
\end{itemize}

\subsection{A First Scalable CP Based Implementation}

As mentioned earlier in this paper, Spark's original algorithm is composed of two different execution stages. We thus analysed each stage independently to understand how CP technique could improve their implementation. Although we rapidly discovered that the scalable stage could hardly be modified to efficiently incorporate CP techniques, at least not without completely incorporating Spark into the solver, we realised that the local execution stage could be significantly improved. \newline

In fact, the entire local execution could be easily replaced by a CP based algorithm. For \acrshort{SoS} problems where PPIC is applicable, we could even use a nearly identical implementation where only the pre-processing stage would need to be modified, as to fit Spark's middle-put. \newline

To remain able to solve \acrshort{SoSS} problems through a local execution. A simple boolean was also added, so that users could specify whether PPIC could be used on the input dataset. In case it couldn't be used, the original local execution of Spark would be used. \newline

This first implementation's pseudo-code can be found below, in Algorithm \ref{alg:FirstImplem}.

\subsubsection{Improvements Pathways}

To improve this first scalable CP based implementation, we identified three promising options that needed to be studied:

\begin{enumerate}
	\item Improve the link between Spark's middle-put and PPIC's input. The easiest option, but also the most promising, since both algorithms have been separately optimised in terms of performances.
	\item Improve Spark's performances by incorporating more ideas from pattern mining.
Which would improve both the scalable execution and the original local execution component of Spark. Thus improving performances on both single and multi-item pattern problems.
	\item Developing a new version of PPIC which can efficiently be applied to multi-item pattern. Making CP usable for every local execution opportunity and, hopefully, improving performances.
\end{enumerate}

Additionally to those performance improving options, we also decided to prove that PPIC's modularity can be conserved in a scalable environment through the addition of multiple  functionalities in our future implementations.

\subsection{Adding new functionalities}

To first task we undertook was to add four new functionalities to our first CP based Implementation. The implemented functionalities were as follows:

\begin{enumerate}
	\item Unbounded max pattern length: Although Spark's implementation already disposed of a way to control the maximum length of a pattern, no special value existed to allow unlimited max pattern length. We thus added this minor functionality, specifying 0 as a special value that would allow searching for all solutions pattern of any length. Additionally, we changed the default value of 10 to this new special value, so that all solutions could be found by default.
	\item Min pattern length: Although Spark's original implementation allowed to control the maximal length of a pattern. No such functionalities existed to control their minimal length. We thus added a functionality to specify the minimal length a pattern should have before being considered solution. As all patterns containing fewer non-zero items than the specified input wouldn't be outputted, we decided to set the default value of this parameter at 1, so that the returned solutions wouldn't be restricted.
	\item Limit on the maximal number of items per item-set: This functionality was added so that users could, once again, better control their outputted results. Supposing an hypothetical business would like to find all sequences of items bought in pairs in a dataset, it would have no need for solutions where item-sets are larger than two. We can thus stop searching for further item-set extensions once the limit has been reached and, de facto, improve our algorithm performances for returning those specific solutions. Additionally, we created a special value (0) so that all item-set of any length could be outputted, and set that special value as our default for this parameter.
	\item Soft limit on the number of sub-problems created: By default inactivated, this parameter is enabling far better performance on protein like datasets where projected databases tend to have rather similar sub-problems. Its implementation is a simple check at the beginning of each iteration of the scalable algorithm. If the number of sub-problems created is larger than the user inputted value, the implementation will forcefully put an end to the scalable stage, and switch to a local execution on each worker.
	For the default value of this parameter, a special value (0) was created so that the number of created sub-problems wouldn't be limited.\newline
	
	 As you can see in Figure \ref{fig:sub_problem_limit}, performance improvement are only observed on the protein, Kosarak, slen2 and slen3 datasets, but the increase in performance is significant. The problem being that the loss of performance on other datasets is generally even more significant. This loss in performance comes from large differences in sub-problem sizes appearing due to the forced local execution. Since the largest problems tend to be created and processed last, only a few executors have problems to work on toward the end of the execution. The remaining executor staying idle for the reminder of the local execution stage. Moreover, those remaining problems take a very long time to compute, greatly slowing down the measured performance. \\
	 As is, this parameter should be used with extreme care. However, we will consecrate an implementation design to improve this improvement's performances later in this paper.
\end{enumerate}

While these additions did not have any measurable impact on performances at their default value, they still allowed better control of the search space. Proving that, although using a CP solver in Spark seriously affected modularity, a certain level could be easily kept from the original CP solver. \newline

A pseudo-code demonstrating the changes brought to the code can be found in Algorithm \ref{alg:NF}. \newline

We then looked into improving our performances, leading to the discovery of two potential inefficiency.

\subsubsection{Quicker - Start}

During the pre-processing of Spark's original implementation, unfrequent items are cleaned from the database. Frequent items are thus found in the process, only to be discarded and searched for once more when projecting an empty prefix at the beginning of the scalable execution. \newline

We thus modified our first CP implementation to remove this 'inefficiency', deciding to pass frequent items directly to the scalable stage, instead of discarding them before searching for them once more through another complete iteration over the database. \newline

The code was modified accordingly, as shown in Algorithm \ref{alg:QuickStart}.

\subsubsection{Cleaning Sequence before the Local Execution}

The next inefficiency we found was that, during PPIC's local execution, three matrices were built whose size depended on the number of  unique symbols in the input databases.
Yet, in our first implementation, the various projected Databases that reached the local execution stage often had unfrequent symbols that could potentially be cleaned. \newline

Cleaning them would not only reduce the input database's size, it would also reduce the size of those matrices. Making it a potentially worthwhile deal to pre-process PPIC's input for each projected database of the local execution. \newline

We thus modified our code accordingly, producing the code found in Algorithm \ref{alg:CleanBeforeLocalPPIC} \newline

Since, as we will see later in the performance testing section, large increase in performances can be observed through these two improvements, and since so many other improvements needed to be implemented, \textbf{we decided to use this implementation as reference for the remainder of this paper}. All further improvements were thus added separately to this version, since it would later allow us to better compare the performance gains brought by each implementation. \newline

We will thus end this implementation section with a final algorithm regrouping all implementation which showed performance improvements.

\subsection{Improving the Switch to a CP Local Execution}

In this section, we will discuss the improvement we tried to bring to the translation from Spark's middle-put to our local executions input, and the results we obtained. For each improvement, we will explain its nature and the trade-off's its implementation may encompass.

\subsubsection{Automatic Choice of the Local Execution Algorithm}

Our next attempted improvement was to automate the choice between PPIC and Spark's local execution. Using the former only for \acrlong{SoS} problems, and the latter for \acrlong{SoSS} problems. \newline

This would allow to be more efficient by default, without needing involvement from the user to decide whether PPIC or Spark's original local execution should be used during the local-execution stage. \newline

In that endeavour, we decided to recalculate the 'maxItemPetItemSet' argument dynamically during the pre-processing stage. Since it allows us to determine whether we were dealing with a \acrshort{SoS} or \acrshort{SoSS} database, and thus, whether PPIC could be used during the local execution. \newline

Additionally, should 'maxItemPerItemSet' be left to its default value or should it have been put to much too high a value, we could theoretically slightly improve performance by refraining from searching extensions to solution patterns having already reached the max recalculated length. Of course, this theoretical performance gain would only apply for databases where the number of Items per item-set are mostly constant.\newline

But this additional feature would be worth a small loss of performance on \acrshort{SoSS}, as long as the usage of PPIC could be guaranteed on \acrshort{SoS} problems. \newline

Of course, should the non-recalculated parameters be used with anything but it's default value, the algorithm would make sure only the requested solutions would be computed. \newline

A pseudo-code demonstrating the implementation of this automatic choice can be found in Algorithm \ref{alg:AutomaticSelection}

\subsubsection{Generalised Pre-Processing Before the Local Execution}

Extending our previous idea of cleaning the sequence database before PPIC's execution, we decided to create an implementation where the database would be cleaned before any local execution, including \acrshort{SoSS} problems. Wondering, if cleaning the sequence database before Spark's original local execution could bring similar improvements. \newline

We thus designed a new cleaning process suited both for Spark and PPIC's local execution input. During this process, we also realised we could also easily check whether the cleaned sequences could be solved using PPIC or Spark. We thus decided to include this feature too, and to compare its performance gain to our previous 'maxItemPetItemSet' based implementation. \newline

While creating this new implementation, we also discovered more than a few inefficiency on the old one. Such as the use of ArrayBuffer structure instead of the much more efficient ArrayBuilder, or un-needed extra iteration during the matrices creation. \newline

We thus expected this new implementation to be more efficient on both types of sequential pattern mining problems and, hopefully, for every dataset. \newline

A pseudo-code representing this implementation can be found in Algorithm \ref{alg:CleanBeforeLocal}.

\subsection{Improving the Scalable Execution}

In this section, we will discuss the improvement we tried to bring to Spark's execution stages, and the results we obtained. For each improvement, we will first explain its nature and the trade-off's its implementation may encompass.

\subsubsection{Position Lists}

The first idea we had to improve Spark's performances was to use LAPIN's position list to our advantage. \newline

In Spark's original implementation, the scalable and local stages of the algorithm perform their duty in three phases. First they receive a solution prefix which needs to be extended, and project it on the whole database, allowing them to know which sequence support that prefix. Then, in the supporting sequences only, they search for symbols which may extend the prefix. Finally, if some symbols are found and they respect the constraint applied to the solutions, Spark's will save them as solutions and try to extend them further. \newline

While this implementation is very efficient in a scalable environment, we thought it could be improved through the addition of position list. More specifically, during the prefix projection phase, we determined it would improve performance if the algorithm knew earlier when a sequence couldn't possibly hold the currently projected pattern, or if the algorithm didn't have to analyse half of the sequence before starting to project this aforementioned pattern. \newline

Of course, the trade-off would be a more important use of memory, as the positions list would need to be kept on RDD, along-side their corresponding sequences. To adopt this solution, the measured performance improvement would thus need to be important enough to motivate the benefits of the trade-off. \newline

From this idea, we created three new implementations. The first using only a last position list, the second using only a first position list, and the last using both position-lists together. \newline

In Algorithm \ref{alg:posLists}, you will find a pseudo-code of the implementation including both positions lists together. To obtain the two other implementation, simply remove all pieces of code concerned with either the firstPositionList or lastPositionList.

\subsubsection{Specialising the Scalable Execution}

To improve the scalable execution's performances further, we then had the idea to separate the scalable execution stage of \acrshort{SoS} and \acrshort{SoSS} problems. \newline

This idea stemming that, for \acrshort{SoS} problems, the database's internal representation could be seriously shortened by removing unnecessary separators, effectively reducing the size of their internal representation by two. Furthermore, this more compact representation would allow us to switch twice sooner to the local execution step, as the projected database is now smaller. \newline

We thus implemented a new scalable stage specialised for \acrlong{SoS} problems. Its main features being the absence of spacial start and the much more compact database representation without delimiters. \newline

A trade-off was however made, as we now needed to detect in which type of pattern mining problem we were before starting the execution. Since we now needed to create a different internal representation depending on the database's type, and since simply removing the delimiters after a database type check wouldn't be efficient. \newline

The most efficient way we found to detect the type of problem was during the frequent symbol detection part of the pre-processing step, where through a simple modification we could efficiently detect the type of each sequence, and thus whether we could use the shortened \acrshort{SoS} representation on the database. \newline

We thus implemented the code found in Algorithm \ref{alg:SpecializedScalableExec} to successfully put in practise this idea.

\subsubsection{Priority Scheduling for Sub-Problems}

The final idea we explored to improve Spark's performances, was to modify the order in which sub-problems are computed during the local execution stage. \newline

In the original code, problems are decomposed until they become smaller than a size specified by the 'maxLocalProjDBSize' parameter. As mentioned before, in our reference algorithm, an extension of that idea, the 'subProblemLimit' parameter, was also implemented to allow better control of the number of sub-problems created. \newline

However, we have seen that a consequence of this new functionality is that sub-problems can largely vary in size, making some problem far harder to solve than others. Something which would rarely appear in the original version, unless the maxLocalProjDBSize parameter was put way past its default value. Coincidentally, we also realised that major drops in performance were experienced if those large problems were solved last, since some executor would be left with nothing to do while others would be stuck with catastrophically large workload.
\newline

The solution was clear, large problems needed to be solved first in the various executor, so that smaller workload could be shuffled between executor in the later stage of the execution.

\paragraph{Analysing sub-problem creation}

The piece of code which created the various sub-problems from the various projected prefixes collected during the scalable stage was fundamentally a mapReduce process. The sequences from the original sequence database being projected one by one with different prefixes, then mapped to some reducer, depending on the prefix's ID 
(see Figure \ref{fig:sparkMapReduce}). \newline

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{sparkMapReduce.png}
  \caption{Spark's mapReduce}
  \label{fig:sparkMapReduce}
\end{figure}

\paragraph{Sort sub-problem on the reducer}

We thus concluded that a simple solution could be implemented. According to the specification of Spark's sortBy function, the sorting stage will be exacted locally on each reducer. We thus implemented this quick change by adding a simple sort between our map and reduce stage, obtaining the implementation found in Algorithm \ref{alg:sortReducer}. \newline

But, as you will see during our performance tests, although this algorithm produced better performance. A memory issue now appeared, to the point of crashing our 10G memory executor. \newline

As it turns out, to sort the various sub-problems depending on their size, Spark's obviously needed to evaluate and simultaneously hold in memory all the sub-problems assigned to an executor. Since those sub-problems were transiently created data, they were not stocked on an RDD, and thus couldn't be stored on disk.

To remain scalable, we thus had to come up with another solution to sort our sub-problems. One that did not involve computing multiple sub-problems while keeping them in memory..

\paragraph{Sort sub-problem on the mapper}

We thus realised we needed to sort our sub-problems during the mapping phase of the mapReduce process. After a few trial and error, we realised that the mapping function of Spark created sub-problems one by one, following the mapping code and sent them directly to the reducer through the groupBy function, which delivered them in the very same order they were sent. \newline

We thus modified our implementation to change the order in which we created our sub-problem, instead of sorting them afterwards. The created sub-problems would now be mapped to the reducer and computed in the same order that they were mapped.\newline

Meaning that, by mapping the hardest problems first, they would also be executed first. \newline

We then modified our code to sort our prefixes in descending order through their projected database size, a size which was already computed during the scalable stage of our algorithm, and could simply be stored in the prefix until used to sort. This made sorting sub-problems far more efficient, at a small cost in memory. \newline

As expected, sorting a few prefixes used an insignificant amount of memory in comparison to sorting huge RDDs containing complete projected database, while producing equivalent, if not better, performances. As you will see in the dedicated performance testing section. \newline

A pseudo-code based on the implemented changes can be found in Algorithm \ref{alg:sortMapper}

\subsection{CP Based Local Execution for Sequence of Sets of Symbols}

Our final implementation attempts were to create a CP-based implementation to solve pattern mining problems involving sets of symbols. Of course, to replace the original local execution, this implementation would need to be more performant than its predecessor.

\subsubsection{Pushing PPIC's Ideas Further}

Our first attempt at creating such an implementation was to try pushing PPIC's ideas further. \newline

We decided to forgo all three pre-computed matrices, and to change the structure of our sequence database to fill those matrices purposes more efficiently. Instead of an array, we changed each sequence into a map containing unique symbols as key and the various positions of each the respective symbol as value. \newline

Additionally, we represented the sequence's symbol positions list (including delimiter position) by a ReversibleArrayStack structure, thus allowing efficient backtracking of a symbol's remaining position  through trailing. \newline

This new structure's purpose was to allow us to make distant jumps and checks more efficiently, at the cost of a slightly higher memory consumption. Theoretically, finding the next position of a given symbol would be O(1), be this next, last or first position of the symbol. Should a symbol's position list be empty, or should the last recorded position be smaller than our current position in the sequence, we would also immediately know that no more occurrences of said symbol were contained in the remains of the sequence. \newline

The trade-off lied in the number of reversible points that needed to be maintained. With one ReversibleArrayStack per symbol for each sequence. This number could grow quite quickly, the results may thus greatly vary between datasets but we had good faith it the performance tests would yield satisfying results. \newline

Translating our improvement to pseudo-code, we obtain Algorithm \ref{alg:PPICMAP}.

\subsubsection{Adding Partial Projections to PPIC}

We also decided to try another approach at developing an efficient CP based implementation for problems involving sequences of sets of symbols. This second attempt focusing on bringing the partialStart structure of Spark in a PPIC-like algorithm. \newline

First we realised that keeping all three matrices wouldn't be efficient. In a set of symbols pattern mining context. To keep the same function, the 'interesting position' matrix needed to be modified to indicate the next last appearance of an item in an itemset, instead of the next last appearance of an item in the sequence, as it did before. This in turn, makes this matrix useless as the next such position would nearly always be the next item of the database. \newline

We thus decided to remove this matrix, but to keep the first and last position lists, as they remained relevant in a sequence of sets of symbols context.
We also modified our implementation to keep zero delimiters during cleaning and to change the partial start received from Spark during the pre-processing, so that they still referred to the same item-set after cleaning (lest the item-set completely disappear in which case that particular partial start will be scraped). \newline

The disadvantage of partial starts, however, was that they need to be maintained and advanced at each step of our DFS execution. This meant that, when starting a new item-set, all remaining sequences of the database would need to be fully explored. \newline

For item-set extensions, however, only the position list as partial starts would need to be checked, along the sub-sequent positions appearing before the next separator. We thus expect greater performance improvement the longer item-sets could be extended in the database. \newline

A pseudo-code based on the implemented changes can be found in Algorithm \ref{alg:PPICPARTIAL}.

\subsection{Final implementation}

Our final implementation regroups the characteristics of all performance benefiting algorithm presented above. In this section, we will first present our design choices for this final implementation as well as motivate those choices. We will then present a new functionality added exclusively to this implementation. \newline

A pseudo-code representing this final implementation may be found in Algorithm \ref{alg:FINAL}

\subsubsection{Reaching Greater Performances}

The execution proceeds as follows: First, unfrequent symbols are cleaned from the original database. During the detection process, the algorithm also takes time to detect whether we are dealing with a database of sequences of symbols, or with a database of sequences of sets of symbols.\newline

This allows us to keep the specialised scalable execution improvement which brought great performance improvements to SoS database mining problems. \newline

After that, the scalable execution proceeds normally with the new functionalities added in the reference algorithm. We however removed the quicker-start 'improvement' made before the reference implementation, as it brought negative performance improvements (See 'Performances of Quicker-Start' section for details on its negative performances). \newline

During the scalable execution, the algorithm will store small prefixes for the local execution stages, alongside the size of their projected database. So that, should it be necessary, sub-problems may be sorted before the beginning of the local execution stage. \newline

As condition for triggering an automatic sort of the sub-problem, we decided to use the presence of a non-default value for the 'subProblemSoftLimit' parameter (> 1) or , depending on the type, either a 'maxLocalProjectedDBsize' value twice greater than the default of 32000000 for sequences of sets of symbols, or a 'maxLocalProjectedDBsize' greater or equal to 32000000 for sequences of symbols . \newline

The reason we settled on those conditions is quite straightforward. As you may see in the performance test of our implementation 'Sorting sub-problems on the mapper', sorting may not always be beneficial when sorting sub-problems created with the default-value of 'maxLocalProjectedDBsize'. We thus decided to aim a bit higher, and settled on 64000000, where sorting always shows benefits. \newline

Considering the condition on our 'subProblemSoftLimit' parameter, the choice was even simpler, since this parameter is meant to be used for showing the benefits of the local-execution stage, and since performance results may become hazardous when sub-problems aren't sorted properly. \newline

Finally, before each local execution stage, the projected databases will be cleaned similarly to our 'Generalised Pre-Processing Before the Local Execution' improvement. Of course, with a few minor differences as the sequences representation now differs depending on the database type. \newline

An algorithm will then be chosen to solve the sequential pattern mining sub-problem locally. We settled on using PPIC for problems involving sequences of symbols, and our 'Adding Partial Projections to PPIC' implementation for problems involving sequences of sets of symbols, since it gave far better performances on the slen3 dataset than any of our position list implementation and it allows greater modularity on the constraint that can be applied to our algorithm. \newline

Of course, should a problem involving sequences of sets of symbols become a problem involving only sequences of symbols, an automatic switch to PPIC's local execution would be triggered. Ensuring the most performant algorithm is always in use.

\subsubsection{A New Functionality}

Additionally to our performance related design choices, we also decided to add a new functionality to this final implementation, allowing a user to set precise constraint on the occurrences of symbols in solutions pattern. \newline

In our final implementation, a user may thus input a list of tuples of the form (symbol, relation, count) allowing precise control of the solution patterns returned. \newline

For example, by inputting the tuple ('A', <=, 1), a user can ensure that symbol A won't occur more than once in a solution pattern. If the user inputted tuples allow to rule out the presence of a symbol in solution patterns, for example with a tuple such as ('X', <, 1), our algorithm will automatically brand those items as unfrequent and clean them during the pre-processing stage. As they have no chance of appearing in a solution anyway. \newline

The supported relations being '==', '!=', '<', '>', '>=', and '<='. Thus allowing a wild range of control over the solution pattern returned. Of course, if the user doesn't impose any symbol constraint, the execution proceeds correctly and doesn't restrict the solution space. \newline

As before, this new functionality was also designed to affect performance as little as possible when it is not in use. By default, no constraint are applied on the occurrences of symbols.

\newpage
\addtocontents{toc}{\protect\newpage}
\section{Performances}

To compare the performances of our various implementations, we first need to discuss how those performances were measured, and on which dataset they were measured.

\subsection{Datasets}

For our performance tests, eight datasets were chosen for the different characteristic they displayed. The goal being to prove the efficiency of the developed algorithm in a wide range of situations. The chosen datasets and their characteristic are displayed in Table \ref{tab:datasets}: \newline

\begin{table}[h]
  \centerline{
  \begin{tabular}{| c | l | c | c | c | c | c | c | r |}
  	\hline
  	& Dataset & \#SDB & N & avg(\#S) & avg(\#Ns) & max(\#S) & Sparsity & description \\
  	\hline
  	\multirow{6}{*}{1.} 
  	& BIBLE & 36369 & 13905 & 21.64 & 17.85 & 100 & 1.18 & text \\
  	\cline{2-9}  
  	& FIFA & 20450 & 2990 & 36.24 & 34.74 & 100 & 1.19 & web click stream\\
  	\cline{2-9}  
  	& Kosarak-70 & 69999 & 21144 & 7.98 & 7.98 & 796 & 1.0 & web click stream\\
  	\cline{2-9}  
  	& LEVIATHAN & 5834 & 9025 & 33.81 & 26.34 & 100 & 1.25 & text\\
  	\cline{2-9}  
  	& PubMed & 17237 & 19931 & 29.56 & 24.82 & 198 & 1.17 & bio-medical text\\
  	\cline{2-9}  
  	& protein & 103120 & 25 & 482.25 & 19.93 & 600 & 24.21 & protein sequences\\
  	\hline
  	\multirow{3}{*}{2.} 
  	& slen1 & 50350 & 41911 & 13.24 & 13.24 & 60 & 1.0 & generated dataset\\
 	\cline{2-9}  
  	& slen2 & 47555 & 62296 & 17.97 & 17.97 & 74 & 1.0 & generated dataset\\
  	\cline{2-9}  
  	& slen3 & 287676 & 81368 & 17.07 & 17.07 & 85 & 1.0 & generated dataset\\
  	\hline
  \end{tabular}}
  \caption[Dataset features]{
  	\tabular[t]{@{}l@{}}
  		Datasets features. The datasets of category 1 contain only \acrshort{SoS}, \\
  		while the datasets of category 2 contain only \acrshort{SoSS} \\
  		- \#SDB = number of sequences; \\
  		- N = Number of different symbols in the dataset; \\
  		- \#S = Length of a sequence S; \\
  		- \#Ns = Number of different symbols in a sequence S; \\
  		- Sparsity = $\frac{1}{\#SDB} * \sum \frac{\#S}{\#Ns}$
  	\endtabular
  }
  \label{tab:datasets}
  %\caption{Datasets features. \#SDB = number of sequences; N = Number of different symbols in the dataset; \#S = Length of a sequence S; \#Ns = Number of different symbol in a sequence S; sparsity = $\frac{1}{\#SDB} * \sum \frac{\#S}{\#Ns}$}
\end{table}

As you can see, our datasets vary largely in their characteristic, be it in their sparsity or in the size of their set of symbols. Although the selected datasets focus slightly more on \acrlong{SoS}, since it has been our focus in most of our developed improvement, we also made sure to correctly represent \acrlong{SoSS} with our last three datasets. \newline

\subsection{Number of Partitions}

As seen previously in Figure \ref{fig:hadoopVSspark}, Spark's performances may vary greatly depending on the number of partitions created from the Input dataset.\newline

The number of created partition must thus be carefully selected as, should there be too many partitions created, the algorithm will lose considerable amount of time switching its execution between partitions. Also, should there be too little partition, scalability may be affected, as executor will need to hold large partitions in memory. In extreme cases, Spark's shuffler may even crash while shuffling partitions around. \newline

We have thus decided to keep a constant, carefully selected, number of partitions for each dataset. So that our algorithm's performances can remain comparable while guaranteeing a complete absence of shuffler crash due to too large partitions.\newline

The number of partitions created for each dataset are displayed in Table \ref{tab:numPartitions}. As you can see, the number of created partition is closely related to the database's size.

\begin{table}[h]
  \centering
  \begin{tabular}{| c | c | c |}
  	\hline
  	Dataset & File size (Ko) & Number of partitions \\
  	\hline
  	BIBLE & 3065 & 250 \\
  	\hline
  	FIFA & 2594 &300\\
  	\hline
  	Kosarak-70 & 2166 & 250\\
  	\hline  
  	LEVIATHAN & 713 & 100\\
  	\hline
  	PubMed & 1646 & 200\\
  	\hline
  	protein & 126046 & 5000\\
  	\hline
  	slen1 & 5400 & 500\\
 	\hline 
  	slen2 & 6896 & 500\\
  	\hline
  	slen3 & 39654 & 1000\\
  	\hline
  \end{tabular}
  \caption{Number of partitions used during performance tests}
  \label{tab:numPartitions}
\end{table}

\subsection{Performance Testing Procedure}
\subsubsection{Distribution Choice \& Cluster Architecture}

Our performances will be tested running different custom distribution of Spark, compiled independently for each specific implementation. The goal being to test the performances of an actual distribution containing our changes, rather than simply running the program on an existing distribution. \newline

Although this will take us longer to obtain results, the goal behind this choice, is that our code could then easily be proposed as the standard for future distributions of Spark.  \newline

Additionally to that, unless specified otherwise, our algorithm will be executed on Spark's standalone cluster, in cluster deploy mode (not locally). More specifically, during our tests, we will run a simple architecture composed of a single master and of a worker with four executors, each executor running three threads. A representation of this simple architecture is available in Figure \ref{fig:test_architecture}.\newline

Both the driver and the executors will also dispose of a large amount of memory (10G each) during our tests. As it serves no purpose to limit their abilities when purely comparing performances. \newline

Later in our paper, scalable performances will also be tested in a memory-restricted architecture. The architecture then used shall be specified at the beginning of the concerned section.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.45\textwidth]{test_architecture.png}
  \caption{The simple architecture used during the majority of our tests}
  \label{fig:test_architecture}
\end{figure}

\subsubsection{Program Parameters}

At all time, unless specified otherwise, the program parameters will be kept at their default value, so that differences in parameters will never be reflected in our performance comparisons.\newline

An exception to that rule being the 'maxPatternLength' parameter of Spark, which its default value of 10 doesn't fit our purpose as it prevents all sequences to be found on our largest datasets. We will thus use INTEGER.maxValue instead, so that every solution pattern from our datasets may be outputted.\newline

Also, as mentioned previously, all additional functionalities implemented have been designed to affect performance as little as possible when left to their default value. Their default value will thus similarly be used, unless specified otherwise before the performance test.

\subsubsection{Measurement Span}

For our performance tests, we will measure not only the running time of our algorithm, but also its pre-processing and dataset loading performances. The aim being to develop a new implementation that entirely surpasses the old one.

\subsection{Testing the Implementations}

\subsubsection{Comparing Spark and PPIC's original implementations}

Since PPIC is neither scalable nor concurrent, we will exceptionally use and even simpler architecture for our test on Spark, restraining our worker to a single executor so that both implementation can work under equivalent resources. \newline

As you can see in Figure \ref{fig:originalAlgorithmPerformances}, PPIC greatly overcomes Spark on nearly every dataset given the same resources, the only exceptions being the Kosarak-70 and PubMed dataset. \newline

We can however notice that, on those two datasets, PPIC's performances are extremely stable. As we can see in the performance tests involving lower amount of supporting sequence on PubMed, PPIC becomes the most efficient in the long run. We have no doubt that a similar situation would happen for Kosarak-70, should we have been PPIC the opportunity in our tests. \newline

As said previously, the reason behind those stable performance lies in the required pre-processing of PPIC being quite long. The small difference we can see in those stable performances are thus the running time of the main algorithm itself.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.85\textwidth]{originalAlgorithm.png} \\
  \danger  Test realised on a simplified architecture \danger \\
  \begin{tabular}{c l}
  	 & \\
  	\textbf{PPIC:} & 1 thread  with 10G memory \\
  	\textbf{Spark:} & 1 driver + 1 worker with 1 executor. 10G memory each
  \end{tabular}
  \caption{Original performances of PPIC and Spark}
  \label{fig:originalAlgorithmPerformances}
\end{figure}

\subsubsection{Performances of our First CP Based Implementation}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.80\textwidth]{first_implementation.png} \\
  \danger PPIC's performances were still measured using a single thread and 10G of memory \danger \\
  Other performances represented in this graph were measured using the 4-executor test architecture previously described. \\
  As you can see, when given more resources, Spark's performances slowly start to overcome PPIC's performances on a majority of our datasets. 
  \caption{A first implementation that makes PPIC scalable}
  \label{fig:first_scalable_CPbased_implementation}
\end{figure}

As you can see in figure \ref{fig:first_scalable_CPbased_implementation}, this first implementation, which simple merges the two algorithms by replacing Spark's local execution by PPIC, is already much more efficient on \acrlong{SoS} problems than Spark's original algorithm. We can however notice than the single-thread execution of PPIC sometimes overcome our new scalable implementation, despite the lower amount of resources at his disposal. The worst such case being protein, a dataset on which PPIC remain far superior despite the lower resource it disposes, which may appear disconcerting at first glance. \newline

Another disconcerting thing to notice may be the relative flatness of PPIC's performance measurement. Making it seem as if the decrease in the amount of required support isn't really felt by the algorithm. On Kosarak and PubMed, the performances of PPIC remains worse by a 10x factor which cannot be explained by the lack of resources alone. Similarly, on protein, the performance of our new algorithm remain worse by a 10x factor, despite the additional resources given to its execution. \newline

However, those oddities in our performance measurements are perfectly explainable, they actually result from three major factors:

\begin{enumerate}
\item First, the pre-processing of the original PPIC algorithm wasn't implemented efficiently, probably since only the execution time had been compared on PPIC's original paper. PPIC's true efficiency was thus brought down, while our new implementation wasn't, since this pre-processing was rewritten from scratch to fit Spark's middle-put. This time, with efficiency in mind. Explaining the slow, yet stable, performances of PPIC on datasets like Kosarak and PubMed.

\item Second, as explained earlier, three matrices are built from the original sequence database before the beginning of PPIC's execution. However, in PPIC's original implementation, those matrices could be very large. As their size depends not only on the number of sequences, but also on the number of unique items present in the input database. \newline
	
	However, our first-implementation surprisingly partly solved this problem. Since only the interesting parts of the sequence database are kept before launching the local execution, many items and sequences which appeared in the initial problem don't appear during the local execution. For each sub-problem treated in the local execution, the constructed matrices will thus be much smaller in size. Which explains further our new implementation's efficiency gain on datasets with large amounts of distinct items.
	
\item Finally, our third major factor is a major inefficiency that comes as a by-product of scalability. It appears mostly on datasets composed of a small number of repeated symbols, such as our protein dataset. In those cases, since the various sub-problems created before our local execution are actually very similar to each other, our new implementation will lose an important amount of time recreating the three input matrices before launching computations on those nearly identical sequence databases. While the original PPIC implementation would create those matrices once and use them through the reminder of the execution. \newline

Sadly, this problem cannot be dynamically fixed without seriously affecting scalability or efficiency, as it would require us to compare the projection of multiple prefixes. This means that, either we would have to project the prefixes multiple time to obtain the results of those comparisons, either we would project it once and keep multiple version of the database during comparison, which would be a disaster for memory consumption and scalability.\newline

Fortunately, although a complete dynamic fix of the problem is impractical for the implementation of a scalable and efficient solution. It is possible to give users the possibility to reduce or even negate the effects of this by-product inefficiency, through giving them control over the amount of sub-problem created. Since the less such sub-problems, the less those matrices will need to be recalculated. \newline

In fact, Spark's implementation already contains a way to control the number of sub-problems created, thanks to the 'maxLocalProjDBSize' parameter. Stopping further sub-problem creation from problems that are already below the inputted parameter value in size. A more direct and precise way to control the amount of generated problems was thus added among other functionalities in the Quicker-Start implementation.

\end{enumerate}

On the other hand, we can notice that multi-item performance weren't impacted at all. Which is normal since the modification made only concerned single-item problems. \newline

\subsubsection{Performances of Quicker-Start}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.85\textwidth]{quicker_start.png}
  \caption{The performance impact of not recalculating frequent items}
  \label{fig:new_functionalities_perf}
\end{figure}

As you can see, very small gain in time are generally made. With the exception of the Kosarak-70, slen2 and slen3 datasets, which are our datasets possessing the largest number of unique items after cleaning the database. At first, we believed those small losses of performances were due to the delay needed to transfer the already calculated frequent items to the various executor being larger than the time needed to actually recalculate those items from the small cleaned database. It was only much later, when we added the slen3 dataset, that we realised this interpretation was incomplete.\newline

In the original implementation, since the items need to be recalculated, they will be recalculated only on the sequences contained in the partitions of the executor. In this new version, we force every executor to receive those items and to store prefixes object for them. Those objects then created will then be kept on the executor for the entirety of the execution as, should an RDD need to be recalculated, they would be a necessary input for their recreation. \newline

Meaning that, when the number of items are large, this additional feature will require more memory usage, and more computation time to receive the items and their frequencies and create the prefixes. Especially when, as in our slen3 dataset which is a combination of multiple generated datasets, every partition generally concerns a whole different set of items.\newline

Sadly, we realised this mistake much too late, as only the slen3 dataset proved it's quite disastrous implications and, at the time, we had decided to keep this improvement despite the small inefficiency on Kosarak and slen2. Since other datasets displayed rather interesting efficiency gain at the time. This inefficiency was thus introduced in our reference algorithm, and in nearly all the subsequent implementation. \newline

We will however correct this mistake in our final version of the algorithm, and remove the quicker-start functionality, since the change is harmful to the scalability of the program.

\subsubsection{Performances of Adding Pre-processing Before PPIC's Local Execution}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.85\textwidth]{new_functionalities.png}
  \caption{The performance impact of Database Pre-processing Before PPIC's Execution}
  \label{fig:clean_before_PPIC}
\end{figure}

As you can see in figure \ref{fig:clean_before_PPIC}, this additional pre-processing step allows a 3x performance speed-up on the Bible dataset and a 2x speed-up on Fifa. Lesser performance gains were also observed on protein and the latter stages of PubMed. \newline

However, we can observe worsening performances in smaller datasets and in executions with larger minsup values. Theses losses however, pales in comparison to the performance gain observed. Although at first glance they may seem large on log-scale graph, they do not even exceed ten second, while the performance gains can be counted in hundreds of seconds on Bible and Fifa. \newline

The reason being those performance gains is that the larger the dataset and the lower the minimal amount of support, the longer prefixes will need to be before being executed locally. The larger those prefixes are before local execution, the more items can be cleaned from the sequences, the faster we can search the remains of those very sequences for patterns. Finally, if fewer items are present in our sequences during our local execution of PPIC, the three matrices needed for PPIC's execution can be created faster. Thus, hastening performance another fold. \newline

However, cleaning sequences comes with a variable cost depending on the database size. If, much like Kosarak, the database only contains uncleanable sequences, performances will be slightly worsened. Of course, since this change is only applied before PPIC, the performance measured on our three slen datasets weren't affected. \newline

In the end, since such large increase in performances were observed, and since so many other improvements needed to be tested, \textbf{we decided to use the performance obtained here as reference for the remainder of this paper}. To allow better comparison of performances gains/losses, all further improvements were thus added to this version, and tested separately. Finally, by the end of this paper, a final version containing all changes providing performance improvements will be compiled into a single algorithm, algorithm whose performances will be similarly tested.

\subsubsection{Performances with Automatic choice of Local Execution}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.85\textwidth]{MaxItemPerItemset_recalculated.png}
  \caption[Automatic detection of item-sets type in dataset]{
  	\tabular[t]{@{}l@{}}
		Performance improvement of recalculating 'maxItemPerItemset' to avoid \\
		search for multi-item extension when unnecessary
	\endtabular}
  \label{fig:maxItemPerItemset_recalculated}
\end{figure}

As you can see on figure \ref{fig:maxItemPerItemset_recalculated}, this improvement achieves its goal with a slight performance degradation, due to the need for calculating the 'maxItemPetItemSet' value. \newline

While, at first, we thought those small losses were worth this additional feature, as it guarantees the use of the much more performant PPIC. We, however, found soon after that there was a more efficient way to achieve the same goal, as you will see in the next section.

\subsubsection{Performances of Adding Pre-processing Before Any Local Execution}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.85\textwidth]{clean_before_localExec.png}
  \caption{Efficiency gains of cleaning the sequence database before any local execution}
  \label{fig:cleaning_before_local_exec}
\end{figure}

As you can see in figure \ref{fig:cleaning_before_local_exec}, this implementation was, as expected, much more performant, for both types of datasets. \newline

Advantages are, however, lesser when mining sequences of sets of symbols, as the local execution still relies on Spark's implementation, which doesn't take as much advantages of this additional cleaning step. The various improvement we made for cleaning sequences of symbols, on the other hand, are quite strongly reflected in these new performance measurement, surpassing our expectation on their effectiveness.

While, as said earlier, cleaning before the local execution allows us to detect witch algorithm should take charge the inputted database, another advantage also appeared. Should a sub-problem from a database of sequence of sets of symbols become a database of sequence of symbols through cleaning, its local execution would be switched through PPIC, thus improving the significantly the performance on these projected database. \newline

Sadly those cases should only appear rarely, and depend strongly on the inputted data. Still, it remains a nice addition to have, and which give occasional small boost to performances.

\subsubsection{Performances with Position lists}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{positionLists.png}
  \caption{Performance gain of first/last position lists}
  \label{fig:positionListPerf}
\end{figure}

As you can see in Figure \ref{fig:positionListPerf}, the resulting performances of those three implementations weren't conclusive. On every database of sequence of symbols but protein, which is a very sparse dataset on which this type of techniques excel, the modified algorithms delivered worse performances than our reference algorithm. \newline

Those worsening performance appears because the benefit of those additional computation simply do not have enough time to appear. After being calculated during the pre-processing step, they only stay of use during the scalable execution of our algorithm. Since that, before the local execution, the sequence database will be compressed and the positions list will need to be recalculated. \newline

Another reason for those worsening performance would be the transfer time of the calculated positions list, which can be very large depending on the number of frequent items. The delay imposed by their calculation and their transfer through the executors are thus far from negligible. \newline

However, while suffering from the same afflictions, an improvement in performance can be observed on databases of sequences of sets of symbols, where those techniques weren't already in use during the local execution stage. Especially on the slen1 dataset, where the measured performances improve significantly. \newline

Despite the exception of the slen1 dataset, however, the measured performances gain does not justify a nearly doubled memory consumption (nearly tripled in the case of the 'first\_last\_pos' improvement). Should no better implementation be found, this improvement should thus be restrained to only appear in Spark's local execution, where additional memory consumption won't affect data transfer times. Although the measured performances improvement would thus be reduced, keeping such large RDD during the scalable execution step simply wouldn't make sense for so little gains.

\subsubsection{Performances with Specialised scalable execution}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{scalable_single_item.png}
  \caption{Performance improvement of specialising spark's scalable stage.}
  \label{fig:scalable_single_item}
\end{figure}

As you can see on the slen datasets of Figure \ref{fig:scalable_single_item}, this new implementation introduced a small loss in performance for dataset of sequence of sets of symbols. As it needs to go through the added detection but doesn't profit of the advantages. \newline

However, for sequences of symbols, performances improved greatly. Not only that, but the new internal database representation make much more sense from a memory-consumption point of view. \newline

Thus, we judged these great advantages to be worth the slight loss of performance, and decided to include this improvement in our final implementation.

\subsubsection{Performances of Using Priority Scheduling for the Local Execution}

\paragraph{Performances of Sorting Sub-Problems on the Reducer}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{priority.png}
  \caption[Naive priority scheduling]{
  	\tabular[t]{@{}l@{}}
  		Performance of a 'biggest problem first' scheduling \\
  		executed through the default sortBy() function of Spark.
  	\endtabular
  }
  \label{fig:priority_scheduling_performance_comparison}
\end{figure}

As you can see in Figure \ref{fig:priority_scheduling_performance_comparison}, our first try at this implementation, impressive performance improvements can be observed, especially for the Kosarak, protein, slen2 and slen3 datasets. While BIBLE, FIFA and slen1 suffer from slight performance losses in their later stages, the damage is rather limited in comparison to the otherwise gain. Applying the sort function to the algorithm when unnecessary also isn't too damaging even when sub-problems aren't limited, as we can see with the red line, it should however be avoided in the final version. The sort function should only be called when relevant.\newline

At first glance, this thus looks like a nice little improvement. However, as explained in the implementation section, a huge problem remained. As you can see on protein's measurements, the black line lacks its fourth point. The reason being a constant crash of the algorithm due to a lack of memory ! (The reasons behind this failure is explained in the corresponding implementation section)\newline

Thus motivating our second attempt at implementing this improvement, this time by sorting the problem on the mapper rather than the reducer.

\paragraph{Performances of Sorting Sub-Problems on the Mapper}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{priority_final.png}
  \caption{Performance improvement of sorting sub-problems during map stage.}
  \label{fig:priority_scheduling_improved}
\end{figure}

This new implementation's performances were a huge success, as you can see on Figure \ref{fig:priority_scheduling_improved}. \newline

Not only have we obtained our previous boost in performance, we also successfully avoided any memory consumption problem. This implementation advantages should thus be kept for the final version of our algorithm, since it allows far better performances when using the correct set. However, an automatic detection should be implemented, to detect when sorting sub-problems could yield increased performances, and only sort in these circumstances.

\subsubsection{Performances of using a Map Based Sequence Database Structure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.85\textwidth]{multi_item_map.png}
  \caption[Map based - multi\_item algorithm]{
  	\tabular[t]{@{}l@{}}
  		Performance of a CP based algorithm replacing \\
  		all three pre-processed matrices from PPIC by a Map structure
  	\endtabular
  }
  (Please take notice that we are here comparing performances with the original Spark implementation instead of the reference algorithm. Since the featured implementation doesn't possess the quicker-start improvement and the comparison is, in this case, much more relevant.)
  \label{fig:multi_map}
\end{figure}

As you can see in Figure \ref{fig:multi_map}, the performances of this implementation were terrible. \newline

Profiling our algorithm revealed it came from two major factors. First, as feared, to attain the same purpose as the three matrices with our map structure, we needed many more trailing points which had to be backtracked at each step of the search. Regularly generating significant periods of time were our algorithm did nothing but backtrack the various sequences of its database. \newline

The second factor was solution pruning. In PPIC, pruning our search space could be done by checking the last positions of each item, quickly asserting that if the start of a sequence was after the last position of an item in a sequence, the sequence was no longer supporting the item. \newline

Here, however, the situation is different. To efficiently prune a multi-item pattern problem, checking the last position of items was no longer sufficient. To prune efficiently, each item-set now needed to be checked so that, should it match the current prefix, every possible extensions in that item-set could be have their support increased. \newline

Which amount to this algorithm doing twice the work for the same results. As we found out through our tests, pruning using PPIC's original method (last position list) instead of fully, as spark did, produces better performance. A comparison of the two being available in the annexes, Figure \ref{fig:multi_map_comp}. \newline

Although the performances were better when not fully pruning our search space. Spark's original local execution was still much more efficient.

\subsubsection{Performances PPIC with Partial Projection}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.85\textwidth]{multi_item_cp.png}
  \caption[PPIC with partial starts]{
  	\tabular[t]{@{}l@{}}
  		Performance of a CP based algorithm extending \\
  		PPIC's implementation by maintaining partial starts.
  	\endtabular
  }
  \label{fig:multi_array}
  (Please take notice that we are here comparing performances with the original Spark implementation instead of the reference algorithm. Since the featured implementation doesn't possess the quicker-start improvement and the comparison is, in this case, much more relevant.)
\end{figure}

The performance tests realised at the end of our implementation, shown in figure \ref{fig:multi_array}, revealed improved efficiency in comparison to Spark's original algorithm and our previously created map algorithm on sequence of sets of symbols. On slen3, this implementation was even much more efficient than any performances obtained earlier through the three position lists implementation. \newline

These performances, however, came at the cost of an inefficiency on datasets of sequences of symbols, where performances fail to overcome Spark's standard on anything but the FIFA dataset. \newline

This implementation should thus only be used for dealing with sets of symbols, while sequence of symbols should be found through PPIC, whose efficiency was focused on this kind of problem.

\subsubsection{Performances of our final implementation}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{final_ver.png}
  \caption{Performance of our final implementation}
  \label{fig:final_ver}
\end{figure}

As you can see in Figure \ref{fig:final_ver}, our final implementation is much more performant than both our old reference implementation, and the slightly improved 'Clean before any Local Execution' implementation. Surpassing previous performances on nearly every single dataset, but particularly on protein and Kosarak-70 where the 'Specialised Scalable Execution' improvement was extremely effective. \newline

We can also observe that the performance loss brought from the quicker-start 'improvement' have disappeared on the concerned datasets. Improving performance further on our largest datasets. \newline

We a can, however, notice that performance worsens slightly on the slen1 dataset, a light performance loss easily explained by our choice to use our 'PPIC with Partial Projection' implementation, which showed this exact same loss of performance on slen1 but much greater performance gain on slen3, for the local execution of problems involving sequence of sets of symbols. \newline

Finally we can observe that on the FIFA and BIBLE dataset, our performance tends toward the performances of our 'Clean before any Local Execution' implementation as the number of required support gets lowered. \newline

It thus seems that, on those kinds of datasets, the combined improvement fail to bring out further significant performance gains.
Of course, in comparison to the original performances of Spark, this final implementation's performances remain much better. \newline

Another interesting thing to note is that those performances were measured using the default parameters value. Greater performance improvement may thus be achieved by reducing the number of created sub-problems, since it would mean an earlier usage of our local execution algorithms, which are much more efficient than the scalable stage. \newline

\subsection{Scalability Tests}

To conclude our performance testing section, let us measure the scalability of both Spark's original implementation and of our final implementation. \newline

The tests will be conducted as follows: We will run an architecture containing 1 driver (given 5G memory to make sure all solution could be successfully received), and a variable number of workes. Each worker possessing one executor running a single thread, and disposing of only 1G ram memory. \newline

Sadly, since the architecture on which we can run our tests has limited performances, running more than 10 workers will actually display slightly lowered performances, due to necessary switches between threads caused by an insufficient number of cores. We will thus limit ourselves to testing our implementation on architecture running 1,2,4, and 8 workers respectively, and compare the resulting performances under those conditions. \newline

Additionally, we had to specify a reduced value of 64000 for the 'maxLocalProjectedDatabaseSize' parameter whose default value is set at 32000000, as otherwise it could use too much memory during the local execution step and surpass the 1G ram allocated to our workers.

\subsubsection{Scalability of the Original Implementation of Spark}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{scalability_orig.png}
  \caption{Scalability performances of the original implementation}
  \label{fig:scal_orig}
\end{figure}

As you may see on Figure \ref{fig:scal_orig}, the original implementation of Spark scales really well up to a certain point. We can even notice that between 1,2, and 4 workers, doubling the number of workers means twice as good performances. However, this apparent linear scalability disappears when increasing the number of workers to 8 in our architecture, since few changes can be seen in our measured performances. \newline

The original implementation of Spark is thus quite scalable, but has limitations beyond what can be justified by the limited performances of our architecture. More specifically, it appears that when dealing with greater number of executors, Spark gradually use more and more of its time exchanging information instead of doing work. Thus the observed limitations. \newline

\subsubsection{Scalability of our final implementation}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{scalability_final.png}
  \caption{Scalability performances of our final implementation}
  \label{fig:scal_final}
\end{figure}

As you may see on Figure \ref{fig:scal_final}, our final implementation also shows greatly improved performances in a scalable environment, in comparison to the original algorithm. \newline

Although this implementation similarly shows reduced performance gain as the number of workers increase, it has been greatly mitigated. Our implementation is thus not only more performant in general, but also more scalable. Although not quite to the point of achieving true linear scalability. \newline

The greatest advantage of this new implementation being that we can launch the local execution step faster if given more memory. We thus have a closer relation, beyond simple access time to the hard drive, between allocated memory and performances. \newline

A default, however, would be that there is no way to allocate large sub-problems to executors disposing of extra amount of memory instead of other executor which would have more limitation. The 'maxLocalProjectedDatabaseSize' parameter must thus be chosen while taking into account the lowest amount of memory available to an executor. Otherwise, the algorithm may crash during the local execution stage due to insufficient memory.

\newpage
\section{Conclusion}

In this paper, we have successfully developed a novel, scalable, and
efficient CP-based sequential mining method that outperforms completely the original implementation available on Spark.\newline

To do so, we used a generic map-reduce based scalable PrefixSpan implementation to divide the original problem in appropriately sized sub-problems. We then efficiently solved those sub-problems locally using a standard CP-solver running the PPIC algorithm for databases of sequences of symbols, and a newly designed PPIC-like CP algorithm for database of sequences of sets of symbols. Our various performance analysis then demonstrated that this novel approach not only outperformed Spark's original implementation in memory abundant environments, but also on scalable memory limited environment. \newline

This approach, however, has one important flaw, while we have certainly demonstrated through the addition of multiple new functionalities that a certain level of modularity inherent to CP-solver was kept. It remains a fact that using a CP-solver beneath Spark's scalable framework isn't as modular and flexible as an implementation fully englobed in a CP-solver.

The next step forward in progressing this research should thus be taken by incorporating Spark's parallel execution inside a CP framework. Thus allowing efficient scalable execution on large scale databases \textbf{and} great modularity through the simple addition of constraints.

\clearpage
\nocite{*}
%\addcontentsline{toc}{section}{References}
\bibliographystyle{ieeetr}
\bibliography{ref}

\clearpage
\section{Annexes}

\subsection{Glossary:}

\vspace*{-1.8cm}
\glsaddall
\printglossary[title=,type=\acronymtype]

\subsection{Additional example images}

\begin{figure*}[h]
  \centering
  \begin{subfigure}[t]{0.5\textwidth}
  	\centering
    \includegraphics[width=1\textwidth]{Smatrix.png}
    \caption{S-matrix in original database}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
  	\centering
    \includegraphics[width=0.6\textwidth]{SabMatrix.png}
    \caption{S-matrix in <AB>-projected database}
  \end{subfigure}
  \vspace{5px} \\
  S-M[a, b] = (4, 2, 2) => support(AB) = 4; support(BA) = 2; support((AB)) = 2;
  \caption{Example of an S-matrix for PrefixSpan bi-level projection}
  \label{fig:smatrix}
\end{figure*}

\subsection{Additional Performance Comparisons:}

\begin{figure}[h]
  \centering
  \includegraphics[width=1\textwidth]{PPICvsSpecialised.png}
  \caption{PPIC's performances VS other specialized algorithm}
  \label{fig:PPICvsOther}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{spark_original_fixed_preprocessing.png}
  \caption{Performance improvement of fixing Spark's pre-processing}
  \label{fig:spark_preprocessing_fix}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{sub_problem_limit.png}
  \caption[Performance improvement - soft limit on the number of created sub-problem.]{
  	\tabular[t]{@{}l@{}}
  		Performance improvement - soft limit on the number of created sub-problem. \\ 
  		Tested on the reference (Clean Before Local Exec) implementation.
  	\endtabular
  }
  (P
  \label{fig:sub_problem_limit}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{multi_item_map_improved.png}
  \caption[PPIC with a map structure]{
  	\tabular[t]{@{}l@{}}
  		Performance comparison of a full pruning VS a partial pruning \\ 
  		on the \textrm{multi\_map} implementation.
  	\endtabular
  }
  (Please take notice that we are here comparing to the original Spark implementation instead of the reference algorithm. Since the featured implementation doesn't posses the early-start improvement and the comparison is, in this case, much more relevant.)
  \label{fig:multi_map_comp}
\end{figure}

\clearpage
\subsection{Algorithms:}

\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[h]
	\caption{Prefix Projection Incremental Counting propagator (PPIC)}
    \label{alg:PPIC1}
  
    \KwData{Given a sequence of symbol database (SDB), an array of reversible int representing our search space (P), and minimal number of support ($\theta$)}
    \KwResult{PPIC return the set of frequent sequence of symbol contained in SDB, such that any returned pattern appear at least $\theta$ times in the database.}
    
    \SetKwProg{Fn}{Function}{:}{end}
    \SetKwFunction{Propagate}{propagate}

	\tcp{Init global param:}
	sids = [0, ..., len(SDB)]; poss = [0] * len(SDB); \\
	$\phi = 0$; $\varphi = len(SDB)$ \\
	projFreq = Array such that projFreq[i] contains the number of sequence initially supporting symbol i; \\
	\Fn{\Propagate{$SDB,P,i,\theta$}} {
		\tcp{espilon == special value that ends a valid pattern}
    	\uIf {$P_i$ isBoundTo(epsilon)} {
    		\tcp{A valid pattern has been found, fill remaining space to collect}
    		\tcp{$P_0$ cannot be bound to epsilon}
    	 	\ForEach {j in ${i+1, ..., L}$} {P(j).assign(epsilon)}
    	 	\tcc{When success is returned, the current pattern is collected a solution and we backtrack to the previous node of the search tree.}
    	 	\Return Success
    	}
    	\Else { 
    		\While {$P_i$ isBound and i < len(P)} {
    	 		nSupport = projectAndGetFreqs($SDB, Pi, \theta, sids, poss, \phi, \varphi$) \\
    	 		\If {nSupport < $\theta$} {
    	 			\tcc{When failure is returned, we backtrack to the previous node of the search tree.}
    	 			\Return Failure
    	 		}
    	 		\tcp{projFreq has been updated by the projectAndGetFreqs method.}
    	 		\tcp{Prune domain of $P_{i+1}$}
    	 		pruneDomain($i+1, projFreq$) \\
    	 		i++
    	 	}
    	}
    	\tcc{When suspend is returned, we continue down the search tree.}
    	\Return Suspend
   	}
\end{algorithm}

\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[h]
	\caption{PPIC Continued}
    \label{alg:PPIC2}
    
    \SetKwProg{Fn}{Function}{:}{end}
    \SetKwFunction{Proj}{projAndGetFreqs}
    
   	\Fn{\Proj{$SDB, itemToProject, \theta, sids, poss, \phi, \varphi$}} {
   		projFreq[a] = 0   $\forall a \in \{ 1, ..., nSymbols \}$ \\
   		i = $\phi$; j = $\phi + \varphi$; sup = 0 \\
   		\While{$i < \phi + \varphi$}{
   			sid = sids[i]; pos = poss[i]; seq = SDB[sid] \\
   			\tcc{pos => first position in  a sequence, such that the previous prefix was supported. To project the new item, we only need to search the remains of the sequence.}
   			\tcc{sid => The sequence id of a sequence which supported the previous prefix.}
   			\tcc{sids => A vector starting at position $\phi$ and containing $\varphi$ elements, allowing to only search sequence that supported the previous pattern.}
   			\tcc{seq => A sequence where the new item should be projected.}
   			\If {lastPosMap(sid)(itemToProject) - 1 > pos} {
   				\tcp{Find next position of itemToProject in seq}
   				\uIf {pos < firstPosMap(sid)(itemToProject)} {
   					\tcp{Jump to first occurrence of the item in the sequence}
   					pos = firstPosMap(sid)(itemToProject)
   				}
   				\Else {
   					\tcp{Search for the next position of itemToProject}
   					\While {pos < len(seq) and itemToProject != seq[pos]} {
   						pos = pos + 1
   					}
   				}
   				\tcp{item supported -> update projected database}
				sids[j] = sid; poss[j] = pos + 1; j = j + 1; sup = sup + 1; \\
				posToCheck = interestingPosMap(sid)(pos)
				\While {$posToCheck != 0$} {
					\tcp{Faster than checking lastPosMap}
					symbol = seq[posToCheck - 1]
					projFreqs[symbol] = projFreqs[symbol] + 1
					posToCheck = interestingPosMap(sid)(posToCheck - 1)
				}
   			}
   			i = i+1
   		}
   		$\phi$ = $\phi + \varphi$; $\varphi$ = sup \\
		return projFreqs
   	}
\end{algorithm}

\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[h]
	\caption{Spark's original implementation: Pre-Processing}
    \label{alg:SPARK1}
  
    \KwData{Given a sequence of sets of symbol database (SSDB), a minimal number of supporting sequence ($\theta$), a maximal pattern length (maxPatLen), and a maximal local projected database size (maxLocalSize)}
    \KwResult{Return the set of frequent sequence of sets of symbol contained in SDB. Such that any returned pattern appear at least $\theta$ times in the database.}
    
    \SetKwProg{Fn}{Function}{:}{end}
    \SetKwFunction{Pre}{PreProcessing}

	\Fn{\Pre{$SSDB, maxPatLen, maxLocalSize, \theta$}} {
		\tcp{Clean database through a map reduce phase}
    	freqItems = findFrequentItem(SSDB, $\theta$) \\
    	cleanedSequences = cleanSequenceAndRenameItem(SSDB, freqItems)\\
    	\tcp{Find frequent sequences of sets of symbols}
    	solutionPatterns = scalableExecution(cleanedSequences, maxPatLen, maxLocalSize, $\theta$) \\
    	\tcp{Translate to original items name and Return}
    	\Return translateBackToOriginalItemsName(solutionPatterns)
   	}
\end{algorithm}

\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[h]
	\caption{Spark's original implementation: Scalable execution}
    \label{alg:SPARK2}
    
    \SetKwProg{Fn}{Function}{:}{end}
    \SetKwFunction{Scalable}{scalableExecution}
    
   	\Fn{\Scalable{$SSDB, maxPatLen, maxLocalSize, \theta$}} {
		\tcc{Encapsulate each sequence in a postfix, allowing to keep the current position and partial starts without copying the sequences.}
		postfixes = SSDB.map(seq => PostFix(seq)) \\
		\tcp{Init prefix list and solution list}
		solutionPatterns = Array.empty \\
		smallPrefixes = Array.empty \\
		largePrefixes = [prefix.empty] \\
		\tcp{Start scalable execution}
		\While {largePrefixes is not empty}{
			mapReduceResults = postifixes.flatMap( postfix => \\
				\Indp
				largePrefixes.flatMap( prefix => \\
					\Indp
					extensions = postfix.project(prefix).findPrefixExtension()\\
					extensions.map( (item, postfixSize) => \\
						\Indp
						\tcp{Return a (key, value) pair}
						((prefix.id, item), (1, postfixSize)) \\
						\Indm
					) \\
					\Indm
				) \\
				\Indm
			).reduceByKey($(v1.1 + v2.1, v1.2 + v2.2)$) \tcp{Aggregate values by key}
			.filterByValue($v.1 \geq \theta$) \tcp{Keep key-value pair with enough support}
			\tcp{Empty larger prefixes list}
			largePrefixes.clean()\\
			\tcp{Fill it with new, extended, prefixes}
			\ForEach{((prefiID, extendingItem), (support, projDBSize) $\in$ mapReduceResults}{
				\tcp{Create prefix, add it as solution}
				newPrefix = largePrefixes.getByID(prefixID) += extendingItem \\
				solutionPatterns += newPrefix \\
				\If {len(newPrefix) < maxPatternLength}{
					\tcp{len(newPrefix) == number of non-zero item in prefix}
					\tcp{zero == separator between item-sets}
					\uIf {projDBSize > maxLocalProjDBSize} {
              			largePrefixes += newPrefix
            		} 
            		\Else {
              			smallPrefixes += newPrefix
            		}
				} 
			}
		}
		\If {len(smallPrefixes) > 0} {
			\tcp{For each prefix, project the whole database and solve locally}
			toExecuteLocaly = postifixes.flatMap( postfix => \\
				\Indp
				smallPrefixes.flatMap( prefix => \\
					\Indp
					(prefix.ID, postfix.project(prefix).compress())\\
					\Indm
				)\\
				\Indm
			).groupByKey().flatMap(sequences => \\
				\Indp
				localExecution(sequences, maxPatLen - len(prefix), $\theta$) \\
				\Indm
			) \\
		}
   	}
\end{algorithm}

\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[h]
	\caption{Spark's original implementation: Local execution}
    \label{alg:SPARK3}
    
    \SetKwProg{Fn}{Function}{:}{end}
    \SetKwFunction{Local}{localExecution}
    
   	\Fn{\Local{$SSDB, maxPatLen, \theta$}} {
   		\If {maxPatLen == 0}{\Return Array.empty}
   		\tcp{Find extending item that are sufficiently frequent}
   		counts = Map[Int, Int].empty.withDefaultValue(0) \\
		\ForEach {postfix $\in$ SSDB} {
			\ForEach {(extendingItem, size) $\in$ postfix.findPrefixExtension()} {
				counts(extending) += 1
			}
		}
		counts = counts.filterByValue(val $\geq \theta$) \\
		\tcp{Extend for each item and continue searching}
		solutions = ArrayList.empty[Prefix] \\
		\ForEach {key $\in$ counts} {
			projectedDB = postfixes.project(key)\\
			\ForEach {extension $\in$ localExecution(projectedDB, maxPatLen-1, $\theta$)}{
				solution += key + extensions
			}
		}
		\Return solutions
   	}
\end{algorithm}

\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[h]
	\caption{Spark's original implementation: Project and findPrefixExtension}
    \label{alg:SPARK4}
    
    \SetKwProg{Class}{Class}{:}{end}
    \SetKwProg{Fn}{Function}{:}{end}
    \SetKwFunction{Postfix}{Postfix}
    \SetKwFunction{Project}{project}
    
   	\Class{\Postfix{$sequence$}} {
   	
   		start = 0 \\
   		partialProjections = Array.empty
   		
   		\Fn{\Project(Item)} {
   			\If {start > len(sequence)} {\Return}
   			newPartialProjection = Array.empty \\
   			\uIf {Item extends current Item-set in Prefix} {
   				\ForEach {partial $\in$ partialProjections} {
   					newPos = findNextItemInCurrentItemSet(sequence, partial, Item) \\
   					\If {Found Item in current ItemSet}{
   						newPartialProjection += newPos
   					}
   				}
   				start = findSmallestPosition(newPartialProjection) + 1 \\
   			} 
   			\ElseIf {Item start a new Item-set in Prefix} {
   				\For {Pos $\in \{start, ..., len(sequence) - 1\}$}{
   					\If {sequence[Pos] == Item} {
   						\uIf {first found in loop} {
   							start = Pos + 1
   						}
   						\Else {
   							newPartialProjection += Pos
   						}
   					}
   				}
   			}
   			partialProjections = newPartialProjection
   		}
   		\Fn{\Project(Prefix)} {
   			\ForEach {Item $\in$ Prefix} {
   				this.project(Item)
   			}
   		}
   		\Fn{\Project(findPrefixExtension)} {
   			extendingItems = Set.empty \\
   			\ForEach {partial $\in$ partialProjections} {
   				\tcp{Find items that extend current itemSet}
   				extendingItems ++= findAllItemUntilNextSeparator(sequence, partial)
   			}
   			\For {Pos $\in \{start, ..., len(sequence) - 1\}$}{
   				\tcp{Find items that start a new itemSet}
   				extendingItems += sequence[Pos]
   			}
   		}
   	}
\end{algorithm}

\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[h]
	\caption{First scalable CP based implementation}
    \label{alg:FirstImplem}
    
    \SetKwProg{Fn}{Function}{:}{end}
    \SetKwFunction{Scalable}{scalableExecution}
    
   	\Fn{\Scalable{$SSDB, maxPatLen, maxLocalSize, \theta, hasSetsOfSymbols$}} {
		\tcc{Encapsulate each sequence in a postfix, allowing to keep the current position and partial starts without copying the sequences.}
		postfixes = SSDB.map(seq => PostFix(seq)) \\
		\tcp{Init prefix list and solution list}
		solutionPatterns = Array.empty \\
		smallPrefixes = Array.empty \\
		largePrefixes = [prefix.empty] \\
		\tcp{Start scalable execution}
		\While {largePrefixes is not empty}{
			... \tcp{Same as Spark's original implementation}
		}
		\tcp{Start local execution}
		\If {len(smallPrefixes) > 0} {
			\tcp{For each prefix, project the whole database and solve locally}
			toExecuteLocaly = postifixes.flatMap( postfix => smallPrefixes.flatMap( prefix => \\
					\Indp
					(prefix.ID, postfix.project(prefix).compress())\\
					\Indm
			)).groupByKey().flatMap(sequences => \\
				\tcp{Use argument to determine which local execution to use}
				\uIf {hasSetsOfSymbols} {
					localExecution(sequences, maxPatLen - len(prefix), $\theta$) \\
				}
				\Else {
					sequences = removeSeparator(sequences) \\
					matrices = generatePPICMatrices() \\
					\tcc{PPIC will generate the P vector, add constraint to enforce maxPatLen and then launch the execution, calling propagate in the process.}
					PPIC(sequences, maxPatLen - len(prefix), $\theta$, matrices) \\
				}
			) \\
		}
   	}
\end{algorithm}

\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[h]
	\caption{New functionalities: Scalable execution}
    \label{alg:NF}
    
    \SetKwProg{Fn}{Function}{:}{end}
    \SetKwFunction{Scalable}{scalableExecution}
    
   	\Fn{\Scalable{$SSDB, minPatLen, maxPatLen, maxLocalSize, \theta,$ \\ $limitItemPerItemset, softSubProblemLimit$}} {
		\tcc{Encapsulate each sequence in a postfix, allowing to keep the current position and partial starts without copying the sequences.}
		postfixes = SSDB.map(seq => PostFix(seq)) \\
		\tcp{Init prefix list and solution list}
		solutionPatterns = Array.empty \\
		smallPrefixes = Array.empty \\
		largePrefixes = [prefix.empty] \\
		\tcp{Start scalable execution}
		\While {largePrefixes is not empty}{
			\If {len(largePrefixes) + len(smallPrefixes) $\geq$ softSubProblemLimit > 0} {
				smallPrefixes ++= largePrefixes \tcp{Switch to local execution}
				break
			}
			mapReduceResults = postifixes.flatMap( postfix => \\
				\Indp
				... \tcp{See First Scalable CP based Implementation}
				\Indm
			).reduceByKey($(v1.1 + v2.1, v1.2 + v2.2)$) \tcp{Aggregate values by key}
			.filterByValue($v.1 \geq \theta$) \tcp{Keep key-value pair with enough support}
			\tcp{Empty larger prefixes list}
			largePrefixes.clean()\\
			\tcp{Fill it with new, extended, prefixes}
			\ForEach{((prefiID, extendingItem), (support, projDBSize) $\in$ mapReduceResults}{
				\tcp{Create prefix, add it as solution}
				newPrefix = largePrefixes.getByID(prefixID) += extendingItem \\
				\If {respectConstraints(newPrefix, minPatLen, limitItemPerItemset)}{
					solutionPatterns += newPrefix 
				}
				\If {canExtendedItemRespectConstraint(newPrefix, maxPatLen, limitItemPerItemset}{
					\tcp{Search extensions for this prefix}
					\uIf {projDBSize > maxLocalProjDBSize} {
              			largePrefixes += newPrefix
            		} 
            		\Else {
              			smallPrefixes += newPrefix
            		}
				} 
			}
		}
		\If {len(smallPrefixes) > 0} {
			\tcc{Similar local execution than in First Scalable CP based Implementation, but with additional checks to guarantee that the new constraints are respected.}
			...
		}
   	}
\end{algorithm}

\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[h]
	\caption{Quick - Start: Scalable execution}
    \label{alg:QuickStart}
    
    \SetKwProg{Fn}{Function}{:}{end}
    \SetKwFunction{Scalable}{scalableExecution}
    
   	\Fn{\Scalable{$SSDB, minPatLen, maxPatLen, maxLocalSize, \theta,$ \\ $limitItemPerItemset, softSubProblemLimit, frequentItemAndCount$}} {
		\tcc{Encapsulate each sequence in a postfix, allowing to keep the current position and partial starts without copying the sequences.}
		postfixes = SSDB.map(seq => PostFix(seq)) \\
		\tcp{Init prefix list and solution list}
		solutionPatterns = Array.empty \\
		smallPrefixes = Array.empty \\
		largePrefixes = Array.empty \\
		\ForEach {(item, count) $\in$ frequentItemAndCount} {
			largePrefixes += Prefix(item, count) \tcp{count == nbSupport for item}
		}
		\tcp{Start scalable execution}
		\While {largePrefixes is not empty}{
			\tcp{Project and extend prefix}
		}
		\If {len(smallPrefixes) > 0} {
			\tcp{Project and extend prefix locally}
		}
   	}
\end{algorithm}

\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[h]
	\caption{Clean database before local execution of PPIC:}
    \label{alg:CleanBeforeLocalPPIC}
    
    \SetKwProg{Fn}{Function}{:}{end}
    \SetKwFunction{Scalable}{scalableExecution}
    
   	\Fn{\Scalable{$SSDB, minPatLen, maxPatLen, maxLocalSize, \theta,$ \\ $hasSetsOfSymbols, limitItemPerItemset, softSubProblemLimit, freqItemAndCount$}} {
		\tcc{Encapsulate each sequence in a postfix, allowing to keep the current position and partial starts without copying the sequences.}
		postfixes = SSDB.map(seq => PostFix(seq)) \\
		\tcp{Init prefix list and solution list}
		solutionPatterns = Array.empty \\
		smallPrefixes = Array.empty \\
		largePrefixes = Array.empty \\
		\ForEach {(item, count) $\in$ freqItemAndCount} {
			largePrefixes += Prefix(item, count)\tcp{count == nbSupport for item}
		}
		\tcp{Start scalable execution}
		\While {largePrefixes is not empty}{
			\tcp{Project and extend prefix}
		}
		\If {len(smallPrefixes) > 0} {
			\tcp{For each prefix, project the whole database and solve locally}
			toExecuteLocaly = postifixes.flatMap( postfix => smallPrefixes.flatMap( prefix => \\
					\Indp
					(prefix.ID, postfix.project(prefix).compress())\\
					\Indm
			)).groupByKey().flatMap(sequences => \\
				\tcp{Use argument to determine which local execution to use}
				\uIf {hasSetsOfSymbols} {
					localExecution(sequences, maxPatLen - len(prefix), $\theta$) \\
				}
				\Else {
					sequences = cleanUnFrequentItemsAndRename(sequences) \\
					matrices = generatePPICMatrices() \\
					\tcc{PPIC will generate the P vector, add constraint to enforce maxPatLen and then launch the execution, calling propagate in the process.}
					PPIC(sequences, maxPatLen, minPatLen, maxItemPerItemSet, $\theta$, matrices, prefix) \\
				}
			) \\
		}
   	}
\end{algorithm}

\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[h]
	\caption{Automatic Local Execution Selection:}
    \label{alg:AutomaticSelection}
    
    \SetKwProg{Fn}{Function}{:}{end}
    \SetKwFunction{Scalable}{scalableExecution}
    
   	\Fn{\Scalable{$SSDB, minPatLen, maxPatLen, maxLocalSize, \theta,$ \\ $limitItemPerItemset, softSubProblemLimit, freqItemAndCount$}} {
		\tcc{Encapsulate each sequence in a postfix, allowing to keep the current position and partial starts without copying the sequences.}
		postfixes = SSDB.map(seq => PostFix(seq)) \\
		\tcp{Recalculate MaxItemPerItemset}
		calculatedMaxItemPerItemset = findLargestItemSetSize(SSDB) \\
		\If {limitItemPerItemset == 0 or limitItemPerItemset > calculatedMaxItemPerItemset} {
			limitItemPerItemset = calculatedMaxItemPerItemset
		}
		\tcp{Init prefix list and solution list}
		solutionPatterns = Array.empty \\
		smallPrefixes = Array.empty \\
		largePrefixes = Array.empty \\
		\ForEach {(item, count) $\in$ freqItemAndCount} {
			largePrefixes += Prefix(item, count) \tcp{count == nbSupport for item}
		}
		\tcp{Start scalable execution}
		\While {largePrefixes is not empty}{
			\tcp{Project and extend prefix}
		}
		\If {len(smallPrefixes) > 0} {
			\tcp{For each prefix, project the whole database and solve locally}
			toExecuteLocaly = postifixes.flatMap( postfix => smallPrefixes.flatMap( prefix => \\
					\Indp
					(prefix.ID, postfix.project(prefix).compress())\\
					\Indm
			)).groupByKey().flatMap(sequences => \\
				\tcp{Determine which local execution to use}
				\uIf {calculatedMaxItemPerItemset > 1} {
					localExecution(sequences, maxPatLen - len(prefix), $\theta$) \\
				}
				\Else {
					sequences = cleanUnFrequentItemsAndRename(sequences) \\
					matrices = generatePPICMatrices() \\
					\tcc{PPIC will generate the P vector, add constraint to enforce maxPatLen and then launch the execution, calling propagate in the process.}
					PPIC(sequences, maxPatLen, minPatLen, maxItemPerItemSet, $\theta$, matrices, prefix) \\
				}
			) \\
		}
   	}
\end{algorithm}

\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[h]
	\caption{Clean database before any local execution:}
    \label{alg:CleanBeforeLocal}
    
    \SetKwProg{Fn}{Function}{:}{end}
    \SetKwFunction{Scalable}{scalableExecution}
    
   	\Fn{\Scalable{$SSDB, minPatLen, maxPatLen, maxLocalSize, \theta,$ \\ $ limitItemPerItemset, softSubProblemLimit, freqItemAndCount$}} {
		\tcc{Encapsulate each sequence in a postfix, allowing to keep the current position and partial starts without copying the sequences.}
		postfixes = SSDB.map(seq => PostFix(seq)) \\
		\tcp{Init prefix list and solution list}
		solutionPatterns = Array.empty \\
		smallPrefixes = Array.empty \\
		largePrefixes = Array.empty \\
		\ForEach {(item, count) $\in$ freqItemAndCount} {
			largePrefixes += Prefix(item, count) \tcp{count == nbSupport for item}
		}
		\tcp{Start scalable execution}
		\While {largePrefixes is not empty}{
			\tcp{Project and extend prefix}
		}
		\If {len(smallPrefixes) > 0} {
			\tcp{For each prefix, project the whole database and solve locally}
			toExecuteLocaly = postifixes.flatMap( postfix => smallPrefixes.flatMap( prefix => \\
					\Indp
					(prefix.ID, postfix.project(prefix).compress())\\
					\Indm
			)).groupByKey().flatMap(sequences => \\
				\tcp{Clean sequences}
				(sequences, canUsePPIC) = removeUnFrequentItemsEfficiently(sequences) 
				\tcp{NB: Partial starts must also be modified to remain correct}
				\tcp{Determine which local execution to use}
				
				\uIf {! canUsePPIC} {
					localExecution(sequences, maxPatLen - len(prefix), $\theta$) \\
				}
				\Else {
					sequences = removeSeparators(sequences) \\
					matrices = generatePPICMatrices() \\
					\tcc{PPIC will generate the P vector, add constraint to enforce maxPatLen and then launch the execution, calling propagate in the process.}
					PPIC(sequences, maxPatLen, minPatLen, maxItemPerItemSet, $\theta$, matrices, prefix) \\
				}
			) \\
		}
   	}
\end{algorithm}

\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[h]
	\caption{Positions lists: Project and findPrefixExtension}
    \label{alg:posLists}
    
    \SetKwProg{Class}{Class}{:}{end}
    \SetKwProg{Fn}{Function}{:}{end}
    \SetKwFunction{Postfix}{Postfix}
    \SetKwFunction{Project}{project}
    
   	\Class{\Postfix{$sequence$}} {
   	
   		start = 0; partialProjections = Array.empty \\
   		firstPosList = getFirstPosList(sequence); lastPosList = getLastPosList(sequence)
   		
   		\Fn{\Project(Item)} {
   			\If {start > len(sequence) or start > lastPosList(Item)} {\Return}
   			\tcp{Here, we know Item is present in remains of sequence}
   			newPartialProjection = Array.empty \\
   			\uIf {Item extends current Item-set in Prefix} {
   				\ForEach {partial $\in$ partialProjections} {
   					newPos = findNextItemInCurrentItemSet(sequence, partial, Item) \\
   					\If {Found Item in current Item-set}{
   						newPartialProjection += newPos
   					}
   				}
   				start = findSmallestPosition(newPartialProjection) + 1 \\
   			} 
   			\ElseIf {Item start a new Item-set in Prefix} {
   				\If {start == 0} {
   					start = firstPosList(sequence)
   				}
   				\For {Pos $\in \{start, ..., len(sequence) - 1\}$}{
   					\If {sequence[Pos] == Item} {
   						\uIf {first found in loop} {
   							start = Pos + 1
   						}
   						\Else {
   							newPartialProjection += Pos
   						}
   					}
   				}
   			}
   			partialProjections = newPartialProjection
   		}
   		\Fn{\Project(Prefix)} {
   			\ForEach {Item $\in$ Prefix} {
   				this.project(Item)
   			}
   		}
   		\Fn{\Project(findPrefixExtension)} {
   			extendingItems = Set.empty \\
   			\ForEach {partial $\in$ partialProjections} {
   				\tcp{Find items that extend current itemSet}
   				extendingItems ++= findAllItemUntilNextSeparator(sequence, partial)
   			}
   			\For {(Item, lastPos) $\in$ lastPosList, such that lastPos $\geq$ start}{
   				extendingItems += Item
   			}
   		}
   	}
\end{algorithm}

\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[h]
	\caption{Specialised Execution: Pre-Processing}
    \label{alg:SpecializedScalableExec}
    
    \SetKwProg{Fn}{Function}{:}{end}
    \SetKwFunction{Pre}{PreProcessing}
    \SetKwFunction{Scalable}{scalableExecution}

	\Fn{\Pre{$SSDB, maxPatLen, maxLocalSize, \theta$}} {
		\tcp{Clean database through a map reduce phase}
    	freqItems = findFrequentItem(SSDB, $\theta$) \\
    	cleanedSequences = cleanSequenceAndRenameItem(SSDB, freqItems)\\
    	\tcc{Find database type. If any item-set of size greater than one found in sequences, database type will be SSDB, else SDB}
    	databaseType = SSDB.map(sequence => findType(sequence)).reduce() \\
    	\tcp{Find frequent sequences of sets of symbols}
    	solutionPatterns = scalableExecution(cleanedSequences, databaseType, maxPatLen, maxLocalSize, $\theta$) \\
    	\tcp{Translate to original items name and Return}
    	\Return translateBackToOriginalItemsName(solutionPatterns)
   	}
   	\Fn{\Scalable{...}} {
   		\tcc{Separate postfix object in two sub-objects, one which search for item-sets extension (type == SSDB) and one where it never does so (type == SDB). Initialise all sequences with one object or the other, depending on database type.}
   		...
   	}
\end{algorithm}

\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[h]
	\caption{Sorting sub-problems on the reducer:}
    \label{alg:sortReducer}
    
    \SetKwProg{Fn}{Function}{:}{end}
    \SetKwFunction{Scalable}{scalableExecution}
    
   	\Fn{\Scalable{$SSDB, minPatLen, maxPatLen, maxLocalSize, \theta,$ \\ $hasSetsOfSymbols, limitItemPerItemset, softSubProblemLimit, freqItemAndCount$}} {
		\tcc{Encapsulate each sequence in a postfix, allowing to keep the current position and partial starts without copying the sequences.}
		postfixes = SSDB.map(seq => PostFix(seq)) \\
		\tcp{Init prefix list and solution list}
		solutionPatterns = Array.empty \\
		smallPrefixes = Array.empty \\
		largePrefixes = Array.empty \\
		\ForEach {(item, count) $\in$ freqItemAndCount} {
			largePrefixes += Prefix(item, count)\tcp{count == nbSupport for item}
		}
		\tcp{Start scalable execution}
		\While {largePrefixes is not empty}{
			\tcp{Project and extend prefix}
		}
		\If {len(smallPrefixes) > 0} {
			\tcp{For each prefix, project the whole database and solve locally}
			toExecuteLocaly = postifixes.flatMap( postfix => smallPrefixes.flatMap( prefix => \\
					\Indp
					(prefix.ID, postfix.project(prefix).compress())\\
					\Indm
			)).groupByKey()\textbf{.SortByDBSize()}.flatMap(sequences => \\
				\tcp{Use argument to determine which local execution to use}
				\uIf {hasSetsOfSymbols} {
					localExecution(sequences, maxPatLen - len(prefix), $\theta$) \\
				}
				\Else {
					sequences = cleanUnFrequentItemsAndRename(sequences) \\
					matrices = generatePPICMatrices() \\
					\tcc{PPIC will generate the P vector, add constraint to enforce maxPatLen and then launch the execution, calling propagate in the process.}
					PPIC(sequences, maxPatLen, minPatLen, maxItemPerItemSet, $\theta$, matrices, prefix) \\
				}
			) \\
		}
   	}
\end{algorithm}

\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[h]
	\caption{Sorting sub-problems on the mapper:}
    \label{alg:sortMapper}
    
    \SetKwProg{Fn}{Function}{:}{end}
    \SetKwFunction{Scalable}{scalableExecution}
    
   	\Fn{\Scalable{$SSDB, minPatLen, maxPatLen, maxLocalSize, \theta,$ \\ $hasSetsOfSymbols, limitItemPerItemset, softSubProblemLimit, freqItemAndCount$}} {
		\tcc{Encapsulate each sequence in a postfix, allowing to keep the current position and partial starts without copying the sequences.}
		postfixes = SSDB.map(seq => PostFix(seq)) \\
		\tcp{Init prefix list and solution list}
		solutionPatterns = Array.empty \\
		smallPrefixes = Array.empty \\
		largePrefixes = Array.empty \\
		\ForEach {(item, count) $\in$ freqItemAndCount} {
			\tcp{For quick-started prefixes, set projected DB size to max allowed}
			largePrefixes += Prefix(item, count, Long.MaxValue)
		}
		\tcp{Start scalable execution}
		\While {largePrefixes is not empty}{
			\tcp{Project and extend prefix}
		}
		\If {len(smallPrefixes) > 0} {
			\tcp{For each prefix, project the whole database and solve locally}
			\tcp{Prefix are modified to hold their projected database size}
			toExecuteLocaly = postifixes.flatMap( postfix => smallPrefixes\textbf{.SortByDBSize()}.flatMap( prefix => \\
					\Indp
					(prefix.ID, postfix.project(prefix).compress())\\
					\Indm
			)).groupByKey().flatMap(sequences => \\
				\tcp{Use argument to determine which local execution to use}
				\uIf {hasSetsOfSymbols} {
					localExecution(sequences, maxPatLen - len(prefix), $\theta$) \\
				}
				\Else {
					sequences = cleanUnFrequentItemsAndRename(sequences) \\
					matrices = generatePPICMatrices() \\
					\tcc{PPIC will generate the P vector, add constraint to enforce maxPatLen and then launch the execution, calling propagate in the process.}
					PPIC(sequences, maxPatLen, minPatLen, maxItemPerItemSet, $\theta$, matrices, prefix) \\
				}
			) \\
		}
   	}
\end{algorithm}

\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[h]
	\caption{PPIC with a Map based Structure (partial pruning):}
    \label{alg:PPICMAP}
    
    \SetKwProg{Fn}{Function}{:}{end}
    \SetKwFunction{Proj}{projAndGetFreqs}
    \SetKwFunction{Propagate}{propagate}

	\tcp{Init global param:}
	... \\
	\Fn{\Propagate{$SSDB,P,i,\theta$}} {
		... \tcp{Same propagate as PPIC, but the database is an SSDB instead}
	}
    
   	\Fn{\Proj{$SSDB, itemToProject, \theta, sids, poss, \phi, \varphi$}} {
   		projFreq[a] = 0   $\forall a \in \{ 1, ..., nSymbols \}$; i = $\phi$; j = $\phi + \varphi$; sup = 0 \\
   		\While{$i < \phi + \varphi$}{
   			sid = sids[i]; pos = poss[i]; seq = SSDB[sid] \\
   			\tcp{Get positions of itemToProject in sequence}
   			listSoughtItemPos = seq.get(itemToProject).discardItemsBefore(pos)\\
   			\If {not listSoughtItemPos.isEmpty()} {
   				\tcp{Find next position of itemToProject in seq}
   				\uIf {lastItemInPwasSeparator())} {
   					\tcp{Jump to first occurrence of the item in the sequence}
   					pos = listSoughtItemPos.pop() + 1 \\
   					\tcp{item supported -> update projected database}
					sids[j] = sid; poss[j] = pos; j = j + 1; sup = sup + 1; \\
   				}
   				\Else {
   					\tcc{Search the sequence Item-set by Item-set until all elements of P's current item-set match}
   					\For {position $\in$ listSoughtItemPos}{
   						\If {curItemSetInSeqContainsAllItemOfCurrentItemSetInP(seq, position)} {
   							\tcp{Jump to first occurrence of the item in the sequence}
   							pos = listSoughtItemPos.pop() + 1 \\
   							\tcp{item supported -> update projected database}
							sids[j] = sid; poss[j] = pos; j = j + 1; sup = sup + 1; \\
							Break;
   						}
   					}
   				}
				\If {itemToProject supported in sequence}{
					\tcp{Count support for items (partial pruning)}
					\ForEach {(item, positionsList) $\in$ seq} {
						positionList.discardItemsBefore(pos)\\
						\If {not positionList.isEmpty()}{
							projFreqs[item] = projFreqs[item] + 1
						}
					}
					\tcc{For full pruning, find all item-set in sequence that support current Item-set in P, augment the projected frequency of all item present in those item-sets}
				}
   			}
   			i = i+1
   		}
   		$\phi$ = $\phi + \varphi$; $\varphi$ = sup \\
		return projFreqs
   	}
\end{algorithm}

\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[h]
	\caption{PPIC with partial projections:}
    \label{alg:PPICPARTIAL}
    
    \SetKwProg{Fn}{Function}{:}{end}
    \SetKwFunction{Propagate}{propagate}

   	\Fn{\Proj{$SSDB, itemToProject, \theta, sids, poss, \phi, \varphi, partialProj$}} {
   		projFreq[a] = 0   $\forall a \in \{ 1, ..., nSymbols \}$; i = $\phi$; j = $\phi + \varphi$; sup = 0 \\
   		\While{$i < \phi + \varphi$}{
   			sid = sids[i]; pos = poss[i]; seq = SSDB[sid] \\
   			\uIf {itemToProject == separator} {
   				partialProj = Array.empty \\
   				pos = findNextSeparatorInSequence(seq, pos) \\
   				\If {pos < len(seq)}{
   					\tcp{item supported -> update projected database}
					sids[j] = sid; poss[j] = pos + 1; j = j + 1; sup = sup + 1; \\
					projFreq = findFrequencyOfEachItemFromLastPosList(sid, pos)
   				}
   			}
   			\uElseIf {partialProj.isEmpty} {
   				pos = firstPositionOfItemToProjectInRemainOfSequence(seq, pos)\\
   				partialProj = AllSuccessivePosOfItemToProject(seq, pos)\\
   				\If {pos < len(seq)}{
   					\tcp{item supported -> update projected database}
					sids[j] = sid; poss[j] = pos + 1; j = j + 1; sup = sup + 1; \\
					projFreq = findFrequencyOfEachItemFromLastPosList(sid, pos)
   				}
   			}
   			\Else {
   				newPartialProj = Array.empty \\
   				\ForEach {posToCheck $\in$ partialProj} {
   					posToCheck = findPosOfNextItemToProjectInCurrentItemSet(seq, posToCheck) \\
   					\If {posToCheck < len(seq)}{
   						newPartialProj += posToCheck
   					}
   				}
   				partialProj = newPartialProj \\
   				pos = partialProj.min \\
   				\If {not partialProj.isEmpty} {
   					\tcp{item supported -> update projected database}
   					sids[j] = sid; poss[j] = pos + 1; j = j + 1; sup = sup + 1; \\
					projFreq = searchItemSetOfPartialProjForExtendingItem(seq, partialProj)
   				}
   			}
   			i = i+1
   		}
   		$\phi$ = $\phi + \varphi$; $\varphi$ = sup \\
		return projFreqs
   	}
\end{algorithm}

\clearpage
\RestyleAlgo{boxruled}
\LinesNumbered
\begin{algorithm}[h]
	\caption{Final implementation}
    \label{alg:FINAL}
    
    \SetKwProg{Fn}{Function}{:}{end}
    \SetKwFunction{Scalable}{scalableExecution}
    
   	\Fn{\Scalable{$database, minPatLen, maxPatLen, maxLocalSize, \theta,$ \\ $limitItemPerItemset, softSubProblemLimit, freqItemAndCount, symbolConstraints$}} {
		\tcc{Encapsulate each sequence in a postfix, allowing to keep the current position and partial starts without copying the sequences.}
		postfixes = $\emptyset$ \\
		\uIf {database.type() == SDDB}{database.map(seq => PostFixSSoS(seq))}
		\Else {database.map(seq => PostFixSoS(seq))}
		\tcp{Init prefix list and solution list}
		solutionPatterns = Array.empty \\
		smallPrefixes = Array.empty \\
		largePrefixes = [Prefix.empty] \\
		\tcp{Start scalable execution}
		\While {largePrefixes is not empty}{
			\tcp{Project and extend prefix}
			\tcc{When storing prefixes to the local execution, the algorithm also stores their projected database size, so that they can be sorted accurately later.}
		}
		\If {len(smallPrefixes) > 0} {
			\tcp{For each prefix, project the whole database and solve locally}
			\tcp{Sorting sub-problems is done only when necessary.}
			toExecuteLocaly = postifixes.flatMap( postfix => smallPrefixes\textbf{.SortByDBSizeIfNecessary()}.flatMap( prefix => \\
					\Indp
					(prefix.ID, postfix.project(prefix).compress())\\
					\Indm
			)).groupByKey().flatMap(sequences => \\
				\tcp{Use argument to determine which local execution to use}
				sequences = cleanUnFrequentItemsAndRename(sequences) \\
				matrices = generatePPICMatrices() \\
				\uIf {sequences.type() == SSDB} {
					solutionPatterns ++= PPICwithPartialProjections(sequences, maxPatLen, minPatLen, maxItemPerItemSet, $\theta$, matrices, prefix, symbolConstraints) \\
				}
				\Else {
					solutionPatterns ++= PPIC(sequences, maxPatLen, minPatLen, maxItemPerItemSet, $\theta$, matrices, prefix, symbolConstraints) \\
				}
			) \\
		}
		\Return solutionPatterns
   	}
\end{algorithm}

\clearpage
\section{Acknowledgment}

Computational resources have been provided by the Consortium des quipements de Calcul Intensif (CCI), funded by the Fonds de la Recherche Scientifique de Belgique (F.R.S.-FNRS) under Grant No. 2.5020.11

\thispagestyle{empty}		
% To suppress header and footer on the back of the cover page
% Back cover page
\backcoverpage
\end{document}
